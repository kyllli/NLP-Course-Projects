{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTxMV67kpn9Z"
   },
   "source": [
    "Your task this week will be to train a skip-gram Word2Vec model. You may use the code from lecture as a starting point. However, since lecture focused on the CBOW model, you will have modifications to make to the model architecture and training data preparation.\n",
    "\n",
    "To complete the assignment:\n",
    "\n",
    "- Use the following text to train your model: gutenberg.org/cache/epub/7370/pg7370.txt\n",
    "- Write code to:\n",
    "  - Process your data\n",
    "  - Create the training examples and labels\n",
    "  - Train your model\n",
    "  - Compare a few words to evaluate how well the model learned word representations (are they better than random?)\n",
    "- Describe how the Skip-gram model architecture is different from CBOW, making direct references to your code\n",
    "\n",
    "Note that the skip-gram model takes longer to train so it would be a good idea to use a GPU. A free option is to use Google Collab for a free GPU instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SrJ9H4LRlD9B",
    "outputId": "13b8d1a4-cf99-430d-8c60-1465ca34d2e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import modules & set up logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "import gensim\n",
    "import requests\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from typing import List\n",
    "from collections import Counter, OrderedDict\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26aSskprqmcl",
    "outputId": "2ee8b999-0b41-40fb-ca1e-1502e2f48687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPUs: 22\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_processors = multiprocessing.cpu_count()\n",
    "print(f'Available CPUs: {num_processors}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using xpu as device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "elif torch.xpu.is_available():\n",
    "    device = \"xpu\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} as device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "model_dir = \"./models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False # set to false for full training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    CONTEXT_WINDOW = 2 # the number of words on either side of target word\n",
    "    EMBEDDING_SIZE = 5\n",
    "    MIN_FREQ = 5 # dropping words that appear less than 5 times\n",
    "    BATCH_SIZE = 3\n",
    "    N_EPOCHS = 1\n",
    "else:\n",
    "    CONTEXT_WINDOW = 4 # the number of words on either side of target word\n",
    "    EMBEDDING_SIZE = 100\n",
    "    MIN_FREQ =1 # dropping words that appear less than 5 times\n",
    "    BATCH_SIZE = 64\n",
    "    N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/cache/epub/7370/pg7370.txt\"\n",
    "response = requests.get(url)\n",
    "data: List[str] = response.text.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in the data: 4972\n"
     ]
    }
   ],
   "source": [
    "data = [sentence.split() for sentence in data]\n",
    "data = [line for line in data if line and any(word.strip() for word in line)]  # Remove empty lines\n",
    "print(f\"Number of lines in the data: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(\n",
    "        self,\n",
    "        word_counts: OrderedDict, # vocabular is based on word counts\n",
    "        min_freq: int = 1, # min times a word must appear in corpus (rare words might not be worth considering)\n",
    "        max_size: int = None, # we can limit the amount of words as well \n",
    "        specials: List[str] = None, # any other special tokens we may want to add, like padding tokens\n",
    "        unk_token: str = \"<unk>\" # reserved token for when we run into words not in the vocabulary\n",
    "    ):\n",
    "        self.word_counts = word_counts\n",
    "        self.min_freq = min_freq\n",
    "        self.max_size = max_size\n",
    "        self.unk_token = unk_token\n",
    "        self.specials = list(specials) if specials else []\n",
    "\n",
    "        if self.unk_token not in self.specials:\n",
    "            self.specials.insert(0, self.unk_token) # unknown token should always be included\n",
    "\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = []\n",
    "\n",
    "        self._prepare_vocab()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2token)\n",
    "    \n",
    "\n",
    "    def __contains__(self, value):\n",
    "        return value in self.idx2token\n",
    "\n",
    "\n",
    "    def _prepare_vocab(self):\n",
    "        \"\"\"Processes input OrderedDict: Filters based on min_freq & adds special tokens.\"\"\"\n",
    "        vocab_list = self.specials.copy()  # Copy specials to avoid modifying original list\n",
    "\n",
    "        # filter words based on min_freq and add to vocab\n",
    "        filtered_words = [\n",
    "            word\n",
    "            for word, freq in self.word_counts.items()\n",
    "            if freq >= self.min_freq and word not in self.specials\n",
    "        ]\n",
    "\n",
    "        # enforcing max vocab size constraint\n",
    "        if self.max_size is not None:\n",
    "            n_to_keep = self.max_size - len(self.specials) # special tokens take up spaces\n",
    "            filtered_words = filtered_words[:n_to_keep]\n",
    "\n",
    "        # creating final vocab list\n",
    "        vocab_list.extend(word for word in filtered_words)\n",
    "\n",
    "        # create look up tables\n",
    "        self.idx2token = vocab_list\n",
    "        self.token2idx = {word: idx for idx, word in enumerate(vocab_list)}\n",
    "\n",
    "\n",
    "    def get_token(self, idx: int) -> str:\n",
    "        \"\"\"Returns the token corresponding to an index. Raises error if index is out of range.\"\"\"\n",
    "        if 0 <= idx < len(self.idx2token):\n",
    "            return self.idx2token[idx]\n",
    "        raise IndexError(f\"Index {idx} is out of range for vocabulary size {len(self.idx2token)}\")\n",
    "\n",
    "\n",
    "    def get_index(self, token: str) -> int:\n",
    "        \"\"\"Returns the index corresponding to a token. Defaults to unk_token if missing.\"\"\"\n",
    "        return self.token2idx.get(token, self.token2idx[self.unk_token])  # return unk_token index if word is not in vocab\n",
    "\n",
    "\n",
    "    def get_tokens(self, indices: List[int]) -> List[str]:\n",
    "        \"\"\"Converts a list of indices into a list of tokens.\"\"\"\n",
    "        return [self.get_token(idx) for idx in indices]\n",
    "\n",
    "\n",
    "    def get_indices(self, tokens: List[str]) -> List[int]:\n",
    "        \"\"\"Converts a list of tokens into a list of indices.\"\"\"\n",
    "        return [self.get_index(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(sentences: List[List[str]], context_length: int, pad_token: str = \"<pad>\") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Pads each sentence to fit the context window length with the literal string \"<pad>\".\n",
    "    \n",
    "    Args:\n",
    "        sentences: A list of sentences, where each sentence is a list of tokens.\n",
    "        context_length: The number of tokens to either side of the target token.\n",
    "\n",
    "    Returns:\n",
    "        A list of padded sentences.\n",
    "    \"\"\"\n",
    "    padded_sentences = []\n",
    "    for sentence in sentences:\n",
    "        padded_sentence = [pad_token] * context_length + sentence + [pad_token] * context_length\n",
    "        padded_sentences.append(padded_sentence)\n",
    "    \n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pad_sentences(data, CONTEXT_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(\n",
    "    word_counts=OrderedDict(Counter(chain.from_iterable(sentences))),\n",
    "    min_freq=MIN_FREQ,\n",
    "    specials=[\"<pad>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 7,844\n"
     ]
    }
   ],
   "source": [
    "# creating a vocabulary\n",
    "print(f\"Size of Vocabulary: {len(vocab):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0 corresponds to `<unk>`\n",
      "Index 1 corresponds to `<pad>`\n",
      "Index 5 corresponds to `eBook`\n",
      "Index 100 corresponds to `JOHN`\n",
      "Index 200 corresponds to `WARE,`\n",
      "Index 276 corresponds to `what`\n"
     ]
    }
   ],
   "source": [
    "for idx in [0, 1, 5, 100, 200, 276]:\n",
    "    print(f\"Index {idx} corresponds to `{vocab.get_token(idx)}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Train Data and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skipgram_pairs(sentences: List[List[str]], context_length: int, vocab: Vocab):\n",
    "    \"\"\"\n",
    "    Generate (target, context) pairs for Skip-gram model.\n",
    "    \n",
    "    In Skip-gram, we predict context words from the target word.\n",
    "    Each target word generates multiple training examples (one for each context word).\n",
    "\n",
    "    Args:\n",
    "        sentences: A list of sentences, where each sentence is a list of tokens.\n",
    "        context_length: The number of tokens to either side of the target token.\n",
    "        vocab: a vocab object that maps words to indices and vice versa\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, where each tuple is (target token, context token).\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    contexts = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Convert sentence to indices\n",
    "        enc_sentence = vocab.get_indices(sentence)\n",
    "        \n",
    "        # Each target word will generate multiple (target, context) pairs\n",
    "        for target_idx in range(context_length, len(enc_sentence) - context_length):\n",
    "            target = enc_sentence[target_idx]\n",
    "            \n",
    "            # Words to the left of the target\n",
    "            for j in range(target_idx - context_length, target_idx):\n",
    "                contexts.append(enc_sentence[j])\n",
    "                targets.append(target)\n",
    "            \n",
    "            # Words to the right of the target\n",
    "            for j in range(target_idx + 1, target_idx + context_length + 1):\n",
    "                contexts.append(enc_sentence[j])\n",
    "                targets.append(target)\n",
    "\n",
    "    return torch.tensor(targets), torch.tensor(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cbow_pairs(sentences: List[List[str]], context_length: int, vocab: Vocab):\n",
    "    \"\"\"\n",
    "    Generate (context, target) pairs for CBOW model.\n",
    "\n",
    "    Args:\n",
    "        sentences: A list of sentences, where each sentence is a list of tokens.\n",
    "        context_length: The number of tokens to either side of the target token.\n",
    "        vocab: a pytorch vocab object that maps words to indices and vice versa\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, where each tuple is (context tokens, target token).\n",
    "    \"\"\"\n",
    "\n",
    "    contexts = []\n",
    "    targets = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        # using vocab object to converting the sentence from a list of tokens to a list of integers\n",
    "        enc_sentence = vocab.get_indices(sentence)\n",
    "        \n",
    "        # each sentence can potentially generate several training examples\n",
    "        # target_idx refers to all the position of the target word in the sentence\n",
    "        # <context-word> <context-word> target-idx <context-word> <context-word>\n",
    "        for target_idx in range(context_length, len(enc_sentence) - context_length):\n",
    "\n",
    "            # Create context list and remove target token from it\n",
    "            target = enc_sentence[target_idx]\n",
    "            context = (\n",
    "                enc_sentence[target_idx - context_length : target_idx] # words to the left of the target word\n",
    "                + enc_sentence[target_idx + 1 : target_idx + context_length + 1] # words to the right of the target word\n",
    "            )\n",
    "\n",
    "            # Append the (context, target) pair to the list\n",
    "            contexts.append(context)\n",
    "            targets.append(target)\n",
    "\n",
    "    return torch.tensor(contexts), torch.tensor(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: torch.Size([476448])\n"
     ]
    }
   ],
   "source": [
    "skip_contexts, skip_targets = generate_skipgram_pairs(sentences, CONTEXT_WINDOW, vocab)\n",
    "\n",
    "# note that there are more training examples than sentences\n",
    "# because one sentence, if long enough, can provide mulitple training examples\n",
    "print(f\"Number of training examples: {skip_targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: ['Project']\n",
      "target: ['<pad>']\n",
      "\n",
      "context: ['Gutenberg']\n",
      "target: ['eBook']\n",
      "\n",
      "context: ['of']\n",
      "target: ['Project']\n",
      "\n",
      "context: ['License']\n",
      "target: ['this']\n",
      "\n",
      "context: ['of']\n",
      "target: ['for']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# converting first context-target pair back to string\n",
    "for idx in [10,20,33,446,121]:\n",
    "    print(\"context:\", vocab.get_tokens([skip_contexts[idx].item()]))\n",
    "    print(\"target:\", vocab.get_tokens([skip_targets[idx].item()]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: torch.Size([59556])\n"
     ]
    }
   ],
   "source": [
    "cbow_contexts, cbow_targets = generate_cbow_pairs(sentences, CONTEXT_WINDOW, vocab)\n",
    "\n",
    "# note that there are more training examples than sentences\n",
    "# because one sentence, if long enough, can provide mulitple training examples\n",
    "print(f\"Number of training examples: {cbow_targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: ['Gutenberg', 'eBook', 'of', 'Second', 'of', 'Government', '<pad>', '<pad>']\n",
      "target: ['Treatise']\n",
      "\n",
      "context: ['most', 'other', 'parts', 'of', 'world', 'at', 'no', 'cost']\n",
      "target: ['the']\n",
      "\n",
      "context: ['to', 'this', 'reflection,', 'viz.', 'there', 'cannot', 'be', 'done']\n",
      "target: ['that']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in [6, 27, 1000]:\n",
    "    print(\"context:\", vocab.get_tokens(cbow_contexts[idx].tolist()))\n",
    "    print(\"target:\", vocab.get_tokens([cbow_targets[idx].item()]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset): # subclassing Dataset is required here\n",
    "    \n",
    "    def __init__(self, contexts, targets): # necessary method / function\n",
    "        self.contexts = contexts\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self): # necessary method / function\n",
    "        return len(self.contexts)\n",
    "\n",
    "    def __getitem__(self, idx): # necessary method / function\n",
    "        return self.contexts[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class (similar to CBOW but with target/context swapped)\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, targets, contexts):\n",
    "        self.targets = targets\n",
    "        self.contexts = contexts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.contexts[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, dims=EMBEDDING_SIZE):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=dims)\n",
    "        self.linear = nn.Linear(in_features=dims, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).mean(dim=1) # note which dimension we are taking the mean of\n",
    "        out = self.linear(embeds) # outputting raw logits, number of ouputs == vocabulary size\n",
    "        return out\n",
    "    \n",
    "    def debug_forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        print(\"\\nembeddings shape:\", embeds.shape)\n",
    "        print(embeds)\n",
    "        agg = embeds.mean(dim=1)\n",
    "        print(\"\\nembeddings shape after aggregating:\", agg.shape)\n",
    "        print(agg)\n",
    "        out = self.linear(agg)\n",
    "        print(\"\\nshape of logits:\", out.shape)\n",
    "        print(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW(\n",
      "  (embeddings): Embedding(7844, 100)\n",
      "  (linear): Linear(in_features=100, out_features=7844, bias=True)\n",
      ")\n",
      "Size of Vocabulary: 7,844\n"
     ]
    }
   ],
   "source": [
    "cbow_model = CBOW(vocab_size=len(vocab)).to(device)\n",
    "print(cbow_model)\n",
    "print(f\"Size of Vocabulary: {len(vocab):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1115\n",
      "Epoch 2/10, Loss: 0.0983\n",
      "Epoch 3/10, Loss: 0.0932\n",
      "Epoch 4/10, Loss: 0.0887\n",
      "Epoch 5/10, Loss: 0.0845\n",
      "Epoch 6/10, Loss: 0.0805\n",
      "Epoch 7/10, Loss: 0.0768\n",
      "Epoch 8/10, Loss: 0.0733\n",
      "Epoch 9/10, Loss: 0.0701\n",
      "Epoch 10/10, Loss: 0.0670\n"
     ]
    }
   ],
   "source": [
    "# setting up loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.get_index(vocab.unk_token))\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=0.001)\n",
    "\n",
    "# setting up dataloader\n",
    "dataset = CBOWDataset(cbow_contexts, cbow_targets)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# number of passes through the data\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_contexts, batch_targets in dataloader:\n",
    "        \n",
    "        batch_contexts, batch_targets = batch_contexts.to(device), batch_targets.to(device)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"{batch_contexts=}\")\n",
    "            print(f\"{batch_targets=}\")\n",
    "            \n",
    "            if torch.isnan(batch_contexts).any() or torch.isinf(batch_contexts).any():\n",
    "                print(\"NaN or Inf detected in batch_contexts\")\n",
    "            if torch.isnan(batch_targets).any() or torch.isinf(batch_targets).any():\n",
    "                print(\"NaN or Inf detected in batch_targets\")\n",
    "\n",
    "\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        if debug:\n",
    "            pred = cbow_model.debug_forward(batch_contexts) # use regular forward for training run\n",
    "        else:\n",
    "            pred = cbow_model.forward(batch_contexts)\n",
    "        \n",
    "        loss = criterion(pred, batch_targets)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if debug: break\n",
    "\n",
    "    if debug: break\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{N_EPOCHS}, Loss: {epoch_loss/len(dataset):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip-gram model \n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, dims=EMBEDDING_SIZE):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=dims)\n",
    "        self.linear = nn.Linear(in_features=dims, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Unlike CBOW, we don't average embeddings in Skip-gram\n",
    "        # We simply look up the embedding of the target word\n",
    "        embeds = self.embeddings(inputs)\n",
    "        out = self.linear(embeds)  # outputting raw logits\n",
    "        return out\n",
    "    \n",
    "    def debug_forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        print(\"\\nembeddings shape:\", embeds.shape)\n",
    "        print(embeds)\n",
    "        out = self.linear(embeds)\n",
    "        print(\"\\nshape of logits:\", out.shape)\n",
    "        print(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram(\n",
      "  (embeddings): Embedding(7844, 100)\n",
      "  (linear): Linear(in_features=100, out_features=7844, bias=True)\n",
      ")\n",
      "Size of Vocabulary: 7,844\n"
     ]
    }
   ],
   "source": [
    "skip_model = SkipGram(vocab_size=len(vocab)).to(device)\n",
    "print(skip_model)\n",
    "print(f\"Size of Vocabulary: {len(vocab):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1076\n",
      "Epoch 2/10, Loss: 0.0993\n",
      "Epoch 3/10, Loss: 0.0970\n",
      "Epoch 4/10, Loss: 0.0954\n",
      "Epoch 5/10, Loss: 0.0942\n",
      "Epoch 6/10, Loss: 0.0933\n",
      "Epoch 7/10, Loss: 0.0925\n",
      "Epoch 8/10, Loss: 0.0919\n",
      "Epoch 9/10, Loss: 0.0913\n",
      "Epoch 10/10, Loss: 0.0909\n"
     ]
    }
   ],
   "source": [
    "### training time takes about 20 minutes on T4 gpu\n",
    "\n",
    "# setting up loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.get_index(vocab.unk_token))\n",
    "optimizer = optim.Adam(skip_model.parameters(), lr=0.001)\n",
    "\n",
    "# setting up dataloader\n",
    "dataset = SkipGramDataset(skip_contexts, skip_targets)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# number of passes through the data\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_contexts, batch_targets in dataloader:\n",
    "        \n",
    "        batch_contexts, batch_targets = batch_contexts.to(device), batch_targets.to(device)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"{batch_contexts=}\")\n",
    "            print(f\"{batch_targets=}\")\n",
    "            \n",
    "            if torch.isnan(batch_contexts).any() or torch.isinf(batch_contexts).any():\n",
    "                print(\"NaN or Inf detected in batch_contexts\")\n",
    "            if torch.isnan(batch_targets).any() or torch.isinf(batch_targets).any():\n",
    "                print(\"NaN or Inf detected in batch_targets\")\n",
    "\n",
    "\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        if debug:\n",
    "            pred = skip_model.debug_forward(batch_contexts) # use regular forward for training run\n",
    "        else:\n",
    "            pred = skip_model.forward(batch_contexts)\n",
    "        \n",
    "        loss = criterion(pred, batch_targets)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if debug: break\n",
    "\n",
    "    if debug: break\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{N_EPOCHS}, Loss: {epoch_loss/len(dataset):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cbow_model.embeddings.weight.data, f\"{model_dir}/cbow_weights.pt\")\n",
    "torch.save(skip_model.embeddings.weight.data, f\"{model_dir}/skip_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocab locally\n",
    "with open(f\"{model_dir}/vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Mdoel: compare the trained model with untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_words(embeddings, vocab, word, n=10):\n",
    "    \"\"\"\n",
    "    Find the closest words in terms of cosine similarity.\n",
    "\n",
    "    Args:\n",
    "    model: The trained CBOW model.\n",
    "    vocab: The vocabulary object.\n",
    "    word: The target word.\n",
    "    n: Number of closest words to find (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    A list of tuples containing the closest words and their cosine similarities.\n",
    "    \"\"\"\n",
    "    if word not in vocab.idx2token:\n",
    "        raise ValueError(f\"'{word}' not in vocabulary\")\n",
    "\n",
    "    # Get the index of the word\n",
    "    word_idx = vocab.get_index(word)\n",
    "\n",
    "    # Compute cosine similarity between the word and all other words\n",
    "    word_embedding = embeddings[word_idx]\n",
    "    similarities = F.cosine_similarity(word_embedding.unsqueeze(0), embeddings, dim=1)\n",
    "\n",
    "    # Exclude the word itself\n",
    "    similarities[word_idx] = -1\n",
    "\n",
    "    # Find the top n similar words\n",
    "    closest_idxs = similarities.topk(n).indices\n",
    "\n",
    "    return [(vocab.get_token(idx), similarities[idx].item()) for idx in closest_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbow_weights_from_class = torch.load(f\"{model_dir}/cbow_weights.pt\", weights_only=True, map_location=torch.device(device))\n",
    "skip_weights_from_class = torch.load(f\"{model_dir}/skip_weights.pt\", weights_only=True, map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{model_dir}/vocab.pkl\", \"rb\") as f:\n",
    "    vocab_from_class = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Gutenberg™', 0.6182439923286438),\n",
       " ('group', 0.46330147981643677),\n",
       " ('Gutenberg', 0.45303237438201904),\n",
       " ('attached', 0.4118044972419739),\n",
       " ('Foundation', 0.38001516461372375),\n",
       " ('trademark.', 0.3743043839931488),\n",
       " ('included', 0.3677965998649597),\n",
       " ('electronic', 0.3648824989795685),\n",
       " ('License', 0.36455070972442627),\n",
       " ('concept', 0.35457512736320496)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_words(\n",
    "    embeddings=skip_weights_from_class, # can substitute own weights here\n",
    "    vocab=vocab_from_class, # can substitute own vocab here\n",
    "    word=\"Project\", \n",
    "    n=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ministerially', 0.3565547466278076),\n",
       " ('Chap.', 0.3304622173309326),\n",
       " ('wrought', 0.32428815960884094),\n",
       " ('find,', 0.32320037484169006),\n",
       " ('pleases,', 0.3195438086986542),\n",
       " ('farther,', 0.31941407918930054),\n",
       " ('Rome', 0.31889426708221436),\n",
       " ('different', 0.3183806836605072),\n",
       " ('BEFORE', 0.31094738841056824),\n",
       " ('each', 0.3072778880596161)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_model_untrained = SkipGram(vocab_size=len(vocab))\n",
    "\n",
    "closest_words(skip_model_untrained.embeddings.weight.data, vocab, \"Project\", n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_trained_untrained(embeddings_trained, embeddings_untrained, vocab, target_words, topn=5):\n",
    "    \"\"\"\n",
    "    Compare the closest words for trained and untrained embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings_trained: Trained model embeddings.\n",
    "        embeddings_untrained: Untrained model embeddings.\n",
    "        vocab: Vocabulary object.\n",
    "        target_words: List of target words to evaluate.\n",
    "        topn: Number of closest words to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame showing the closest words for both trained and untrained embeddings.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for word in target_words:\n",
    "        row = [word]\n",
    "\n",
    "        # Get closest words for trained embeddings\n",
    "        try:\n",
    "            trained_sim = closest_words(embeddings_trained, vocab, word, n=topn)\n",
    "            row.extend([f\"{w} ({sim:.6f})\" for w, sim in trained_sim])\n",
    "        except ValueError:\n",
    "            row.extend([None] * topn)\n",
    "\n",
    "        # Get closest words for untrained embeddings\n",
    "        try:\n",
    "            untrained_sim = closest_words(embeddings_untrained, vocab, word, n=topn)\n",
    "            row.extend([f\"{w} ({sim:.6f})\" for w, sim in untrained_sim])\n",
    "        except ValueError:\n",
    "            row.extend([None] * topn)\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    columns = (\n",
    "        [\"Target Word\"] +\n",
    "        [f\"Trained Top {i+1}\" for i in range(topn)] +\n",
    "        [f\"Untrained Top {i+1}\" for i in range(topn)]\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame(rows, columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Target Word",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Trained Top 1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Trained Top 2",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Trained Top 3",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Trained Top 4",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Trained Top 5",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Untrained Top 1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Untrained Top 2",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Untrained Top 3",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Untrained Top 4",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Untrained Top 5",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "9c23e52a-f10f-4eef-85a8-f3f63a0a75b9",
       "rows": [
        [
         "0",
         "Project",
         "Gutenberg™ (0.618244)",
         "group (0.463301)",
         "Gutenberg (0.453032)",
         "attached (0.411804)",
         "Foundation (0.380015)",
         "ministerially (0.356555)",
         "Chap. (0.330462)",
         "wrought (0.324288)",
         "find, (0.323200)",
         "pleases, (0.319544)"
        ],
        [
         "1",
         "slavery",
         "inhabitants. (0.416616)",
         "enters (0.405078)",
         "flattered (0.380136)",
         "chanced (0.375924)",
         "190. (0.372523)",
         "deference (0.338117)",
         "appears (0.330488)",
         "These, (0.322855)",
         "company, (0.321896)",
         "odium (0.319615)"
        ],
        [
         "2",
         "property",
         "wonderful (0.375763)",
         "him? (0.366444)",
         "direct; (0.345035)",
         "writ (0.344350)",
         "favourite (0.339717)",
         "otherwise, (0.351606)",
         "thought (0.349163)",
         "on. (0.328751)",
         "violates, (0.326966)",
         "inroads (0.326171)"
        ],
        [
         "3",
         "war",
         "distinction (0.360338)",
         "orders, (0.353231)",
         "1.F.1. (0.339296)",
         "weal, (0.333891)",
         "pure (0.313938)",
         "inclinations (0.410378)",
         "eBooks. (0.338991)",
         "164. (0.333776)",
         "agreement, (0.333652)",
         "unity (0.330755)"
        ],
        [
         "4",
         "state",
         "hundredth (0.398169)",
         "prevent, (0.373378)",
         "law: (0.365230)",
         "another: (0.356500)",
         "continued, (0.348488)",
         "dangerous, (0.388810)",
         "conjugal (0.376794)",
         "yellow (0.359922)",
         "left; (0.353840)",
         "But, (0.330724)"
        ],
        [
         "5",
         "love",
         "determination (0.441058)",
         "beware (0.412676)",
         "agreeing (0.381267)",
         "consenting (0.364311)",
         "stickler (0.363374)",
         "wilderness (0.375820)",
         "comprehending (0.357659)",
         "children’s (0.343036)",
         "choose (0.342096)",
         "winter, (0.334080)"
        ],
        [
         "6",
         "land",
         "raise (0.370643)",
         "straitening (0.365629)",
         "latter (0.360880)",
         "throne; (0.358619)",
         "Almighty: (0.349587)",
         "slavery, (0.325534)",
         "sacredness (0.317318)",
         "(any (0.317094)",
         "said (0.314947)",
         "defends (0.307193)"
        ],
        [
         "7",
         "owner",
         "Literary (0.449295)",
         "1980. (0.432703)",
         "indemnify (0.403222)",
         "Archive (0.401090)",
         "162. (0.388566)",
         "security: (0.345496)",
         "Justin, (0.342210)",
         "finding (0.332893)",
         "uncultivated (0.325989)",
         "miserable (0.317754)"
        ],
        [
         "8",
         "child",
         "though (0.379086)",
         "lion (0.375819)",
         "devised; (0.365077)",
         "villany. (0.361852)",
         "nourish, (0.358377)",
         "Self-defence (0.402027)",
         "returned (0.372312)",
         "ferro, (0.372173)",
         "comes, (0.353006)",
         "honest (0.327401)"
        ],
        [
         "9",
         "history",
         "For (0.384496)",
         "venison (0.377063)",
         "careful (0.372775)",
         "Cum (0.355754)",
         "righteous (0.355308)",
         "potestate (0.386430)",
         "settled, (0.308138)",
         "offices (0.306163)",
         "222. (0.305195)",
         "secretly (0.300081)"
        ],
        [
         "10",
         "health",
         "bargain (0.387817)",
         "acquiescing (0.365175)",
         "consists, (0.349921)",
         "authority: (0.346687)",
         "12. (0.344153)",
         "dissolved, (0.358068)",
         "passion. (0.334100)",
         "follows: (0.333218)",
         "lived (0.326381)",
         "convoking (0.321636)"
        ],
        [
         "11",
         "legislative",
         "where (0.365921)",
         "die (0.345302)",
         "duration, (0.342903)",
         "freedom, (0.341778)",
         "exception (0.335117)",
         "housing (0.350448)",
         "cured. (0.332248)",
         "vero (0.327208)",
         "admits (0.322830)",
         "PG (0.316128)"
        ],
        [
         "12",
         "federative",
         "vapulo (0.379744)",
         "led (0.369526)",
         "majority: (0.351617)",
         "untie (0.348730)",
         "maturity (0.337431)",
         "prevent (0.345520)",
         "57. (0.339217)",
         "describes (0.334718)",
         "ashes, (0.333056)",
         "eat; (0.332731)"
        ],
        [
         "13",
         "prerogative",
         "week, (0.410894)",
         "French (0.399346)",
         "omnipotency (0.393160)",
         "accounted (0.386962)",
         "age (0.376078)",
         "rebels: (0.478924)",
         "pence (0.348867)",
         "appear (0.337215)",
         "convenient, (0.335632)",
         "heaven; (0.330686)"
        ],
        [
         "14",
         "paternal",
         "Secondly, (0.391736)",
         "mockery (0.375049)",
         "king’s (0.353551)",
         "214. (0.351812)",
         "ruin (0.344638)",
         "advance (0.347834)",
         "system. (0.326646)",
         "ac (0.323806)",
         "per (0.316942)",
         "1.F.4. (0.312978)"
        ],
        [
         "15",
         "political",
         "gross (0.366048)",
         "204. (0.356294)",
         "in: (0.353697)",
         "needed. (0.338409)",
         "miserable, (0.336214)",
         "thoroughly (0.389222)",
         "suprema (0.342478)",
         "paternal, (0.321268)",
         "should, (0.318440)",
         "usefulness (0.311658)"
        ],
        [
         "16",
         "power",
         "power: (0.378586)",
         "stock; (0.375537)",
         "patron (0.343386)",
         "privilege, (0.334428)",
         "excuse, (0.333702)",
         "tributum, (0.336056)",
         "intolerable (0.323507)",
         "(By (0.321822)",
         "propulsandi (0.319499)",
         "succession (0.310854)"
        ],
        [
         "17",
         "tyrant",
         "prescription (0.366177)",
         "soever, (0.364885)",
         "guardians (0.355106)",
         "subject. (0.352139)",
         "consumption, (0.338955)",
         "500_l_. (0.352819)",
         "cause. (0.342023)",
         "2 (0.335136)",
         "injustice, (0.314643)",
         "place (0.313433)"
        ],
        [
         "18",
         "king",
         "England’s; (0.357466)",
         "violences: (0.344591)",
         "debating, (0.335364)",
         "Children (0.332990)",
         "David (0.332505)",
         "since (0.377849)",
         "ages, (0.370721)",
         "senatui (0.352447)",
         "other-wise (0.347251)",
         "acres, (0.325989)"
        ],
        [
         "19",
         "dissolution",
         "violating (0.391697)",
         "forms (0.384087)",
         "called, (0.371473)",
         "practice (0.370252)",
         "variable, (0.369813)",
         "commons: (0.356367)",
         "deliberation. (0.328566)",
         "value (0.315360)",
         "language, (0.311462)",
         "sword, (0.309430)"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target Word</th>\n",
       "      <th>Trained Top 1</th>\n",
       "      <th>Trained Top 2</th>\n",
       "      <th>Trained Top 3</th>\n",
       "      <th>Trained Top 4</th>\n",
       "      <th>Trained Top 5</th>\n",
       "      <th>Untrained Top 1</th>\n",
       "      <th>Untrained Top 2</th>\n",
       "      <th>Untrained Top 3</th>\n",
       "      <th>Untrained Top 4</th>\n",
       "      <th>Untrained Top 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Project</td>\n",
       "      <td>Gutenberg™ (0.618244)</td>\n",
       "      <td>group (0.463301)</td>\n",
       "      <td>Gutenberg (0.453032)</td>\n",
       "      <td>attached (0.411804)</td>\n",
       "      <td>Foundation (0.380015)</td>\n",
       "      <td>ministerially (0.356555)</td>\n",
       "      <td>Chap. (0.330462)</td>\n",
       "      <td>wrought (0.324288)</td>\n",
       "      <td>find, (0.323200)</td>\n",
       "      <td>pleases, (0.319544)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>slavery</td>\n",
       "      <td>inhabitants. (0.416616)</td>\n",
       "      <td>enters (0.405078)</td>\n",
       "      <td>flattered (0.380136)</td>\n",
       "      <td>chanced (0.375924)</td>\n",
       "      <td>190. (0.372523)</td>\n",
       "      <td>deference (0.338117)</td>\n",
       "      <td>appears (0.330488)</td>\n",
       "      <td>These, (0.322855)</td>\n",
       "      <td>company, (0.321896)</td>\n",
       "      <td>odium (0.319615)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>property</td>\n",
       "      <td>wonderful (0.375763)</td>\n",
       "      <td>him? (0.366444)</td>\n",
       "      <td>direct; (0.345035)</td>\n",
       "      <td>writ (0.344350)</td>\n",
       "      <td>favourite (0.339717)</td>\n",
       "      <td>otherwise, (0.351606)</td>\n",
       "      <td>thought (0.349163)</td>\n",
       "      <td>on. (0.328751)</td>\n",
       "      <td>violates, (0.326966)</td>\n",
       "      <td>inroads (0.326171)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>war</td>\n",
       "      <td>distinction (0.360338)</td>\n",
       "      <td>orders, (0.353231)</td>\n",
       "      <td>1.F.1. (0.339296)</td>\n",
       "      <td>weal, (0.333891)</td>\n",
       "      <td>pure (0.313938)</td>\n",
       "      <td>inclinations (0.410378)</td>\n",
       "      <td>eBooks. (0.338991)</td>\n",
       "      <td>164. (0.333776)</td>\n",
       "      <td>agreement, (0.333652)</td>\n",
       "      <td>unity (0.330755)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>state</td>\n",
       "      <td>hundredth (0.398169)</td>\n",
       "      <td>prevent, (0.373378)</td>\n",
       "      <td>law: (0.365230)</td>\n",
       "      <td>another: (0.356500)</td>\n",
       "      <td>continued, (0.348488)</td>\n",
       "      <td>dangerous, (0.388810)</td>\n",
       "      <td>conjugal (0.376794)</td>\n",
       "      <td>yellow (0.359922)</td>\n",
       "      <td>left; (0.353840)</td>\n",
       "      <td>But, (0.330724)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>love</td>\n",
       "      <td>determination (0.441058)</td>\n",
       "      <td>beware (0.412676)</td>\n",
       "      <td>agreeing (0.381267)</td>\n",
       "      <td>consenting (0.364311)</td>\n",
       "      <td>stickler (0.363374)</td>\n",
       "      <td>wilderness (0.375820)</td>\n",
       "      <td>comprehending (0.357659)</td>\n",
       "      <td>children’s (0.343036)</td>\n",
       "      <td>choose (0.342096)</td>\n",
       "      <td>winter, (0.334080)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>land</td>\n",
       "      <td>raise (0.370643)</td>\n",
       "      <td>straitening (0.365629)</td>\n",
       "      <td>latter (0.360880)</td>\n",
       "      <td>throne; (0.358619)</td>\n",
       "      <td>Almighty: (0.349587)</td>\n",
       "      <td>slavery, (0.325534)</td>\n",
       "      <td>sacredness (0.317318)</td>\n",
       "      <td>(any (0.317094)</td>\n",
       "      <td>said (0.314947)</td>\n",
       "      <td>defends (0.307193)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owner</td>\n",
       "      <td>Literary (0.449295)</td>\n",
       "      <td>1980. (0.432703)</td>\n",
       "      <td>indemnify (0.403222)</td>\n",
       "      <td>Archive (0.401090)</td>\n",
       "      <td>162. (0.388566)</td>\n",
       "      <td>security: (0.345496)</td>\n",
       "      <td>Justin, (0.342210)</td>\n",
       "      <td>finding (0.332893)</td>\n",
       "      <td>uncultivated (0.325989)</td>\n",
       "      <td>miserable (0.317754)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>child</td>\n",
       "      <td>though (0.379086)</td>\n",
       "      <td>lion (0.375819)</td>\n",
       "      <td>devised; (0.365077)</td>\n",
       "      <td>villany. (0.361852)</td>\n",
       "      <td>nourish, (0.358377)</td>\n",
       "      <td>Self-defence (0.402027)</td>\n",
       "      <td>returned (0.372312)</td>\n",
       "      <td>ferro, (0.372173)</td>\n",
       "      <td>comes, (0.353006)</td>\n",
       "      <td>honest (0.327401)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>history</td>\n",
       "      <td>For (0.384496)</td>\n",
       "      <td>venison (0.377063)</td>\n",
       "      <td>careful (0.372775)</td>\n",
       "      <td>Cum (0.355754)</td>\n",
       "      <td>righteous (0.355308)</td>\n",
       "      <td>potestate (0.386430)</td>\n",
       "      <td>settled, (0.308138)</td>\n",
       "      <td>offices (0.306163)</td>\n",
       "      <td>222. (0.305195)</td>\n",
       "      <td>secretly (0.300081)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>health</td>\n",
       "      <td>bargain (0.387817)</td>\n",
       "      <td>acquiescing (0.365175)</td>\n",
       "      <td>consists, (0.349921)</td>\n",
       "      <td>authority: (0.346687)</td>\n",
       "      <td>12. (0.344153)</td>\n",
       "      <td>dissolved, (0.358068)</td>\n",
       "      <td>passion. (0.334100)</td>\n",
       "      <td>follows: (0.333218)</td>\n",
       "      <td>lived (0.326381)</td>\n",
       "      <td>convoking (0.321636)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>legislative</td>\n",
       "      <td>where (0.365921)</td>\n",
       "      <td>die (0.345302)</td>\n",
       "      <td>duration, (0.342903)</td>\n",
       "      <td>freedom, (0.341778)</td>\n",
       "      <td>exception (0.335117)</td>\n",
       "      <td>housing (0.350448)</td>\n",
       "      <td>cured. (0.332248)</td>\n",
       "      <td>vero (0.327208)</td>\n",
       "      <td>admits (0.322830)</td>\n",
       "      <td>PG (0.316128)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>federative</td>\n",
       "      <td>vapulo (0.379744)</td>\n",
       "      <td>led (0.369526)</td>\n",
       "      <td>majority: (0.351617)</td>\n",
       "      <td>untie (0.348730)</td>\n",
       "      <td>maturity (0.337431)</td>\n",
       "      <td>prevent (0.345520)</td>\n",
       "      <td>57. (0.339217)</td>\n",
       "      <td>describes (0.334718)</td>\n",
       "      <td>ashes, (0.333056)</td>\n",
       "      <td>eat; (0.332731)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>prerogative</td>\n",
       "      <td>week, (0.410894)</td>\n",
       "      <td>French (0.399346)</td>\n",
       "      <td>omnipotency (0.393160)</td>\n",
       "      <td>accounted (0.386962)</td>\n",
       "      <td>age (0.376078)</td>\n",
       "      <td>rebels: (0.478924)</td>\n",
       "      <td>pence (0.348867)</td>\n",
       "      <td>appear (0.337215)</td>\n",
       "      <td>convenient, (0.335632)</td>\n",
       "      <td>heaven; (0.330686)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>paternal</td>\n",
       "      <td>Secondly, (0.391736)</td>\n",
       "      <td>mockery (0.375049)</td>\n",
       "      <td>king’s (0.353551)</td>\n",
       "      <td>214. (0.351812)</td>\n",
       "      <td>ruin (0.344638)</td>\n",
       "      <td>advance (0.347834)</td>\n",
       "      <td>system. (0.326646)</td>\n",
       "      <td>ac (0.323806)</td>\n",
       "      <td>per (0.316942)</td>\n",
       "      <td>1.F.4. (0.312978)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>political</td>\n",
       "      <td>gross (0.366048)</td>\n",
       "      <td>204. (0.356294)</td>\n",
       "      <td>in: (0.353697)</td>\n",
       "      <td>needed. (0.338409)</td>\n",
       "      <td>miserable, (0.336214)</td>\n",
       "      <td>thoroughly (0.389222)</td>\n",
       "      <td>suprema (0.342478)</td>\n",
       "      <td>paternal, (0.321268)</td>\n",
       "      <td>should, (0.318440)</td>\n",
       "      <td>usefulness (0.311658)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>power</td>\n",
       "      <td>power: (0.378586)</td>\n",
       "      <td>stock; (0.375537)</td>\n",
       "      <td>patron (0.343386)</td>\n",
       "      <td>privilege, (0.334428)</td>\n",
       "      <td>excuse, (0.333702)</td>\n",
       "      <td>tributum, (0.336056)</td>\n",
       "      <td>intolerable (0.323507)</td>\n",
       "      <td>(By (0.321822)</td>\n",
       "      <td>propulsandi (0.319499)</td>\n",
       "      <td>succession (0.310854)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tyrant</td>\n",
       "      <td>prescription (0.366177)</td>\n",
       "      <td>soever, (0.364885)</td>\n",
       "      <td>guardians (0.355106)</td>\n",
       "      <td>subject. (0.352139)</td>\n",
       "      <td>consumption, (0.338955)</td>\n",
       "      <td>500_l_. (0.352819)</td>\n",
       "      <td>cause. (0.342023)</td>\n",
       "      <td>2 (0.335136)</td>\n",
       "      <td>injustice, (0.314643)</td>\n",
       "      <td>place (0.313433)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>king</td>\n",
       "      <td>England’s; (0.357466)</td>\n",
       "      <td>violences: (0.344591)</td>\n",
       "      <td>debating, (0.335364)</td>\n",
       "      <td>Children (0.332990)</td>\n",
       "      <td>David (0.332505)</td>\n",
       "      <td>since (0.377849)</td>\n",
       "      <td>ages, (0.370721)</td>\n",
       "      <td>senatui (0.352447)</td>\n",
       "      <td>other-wise (0.347251)</td>\n",
       "      <td>acres, (0.325989)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dissolution</td>\n",
       "      <td>violating (0.391697)</td>\n",
       "      <td>forms (0.384087)</td>\n",
       "      <td>called, (0.371473)</td>\n",
       "      <td>practice (0.370252)</td>\n",
       "      <td>variable, (0.369813)</td>\n",
       "      <td>commons: (0.356367)</td>\n",
       "      <td>deliberation. (0.328566)</td>\n",
       "      <td>value (0.315360)</td>\n",
       "      <td>language, (0.311462)</td>\n",
       "      <td>sword, (0.309430)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Target Word             Trained Top 1           Trained Top 2  \\\n",
       "0       Project     Gutenberg™ (0.618244)        group (0.463301)   \n",
       "1       slavery   inhabitants. (0.416616)       enters (0.405078)   \n",
       "2      property      wonderful (0.375763)         him? (0.366444)   \n",
       "3           war    distinction (0.360338)      orders, (0.353231)   \n",
       "4         state      hundredth (0.398169)     prevent, (0.373378)   \n",
       "5          love  determination (0.441058)       beware (0.412676)   \n",
       "6          land          raise (0.370643)  straitening (0.365629)   \n",
       "7         owner       Literary (0.449295)        1980. (0.432703)   \n",
       "8         child         though (0.379086)         lion (0.375819)   \n",
       "9       history            For (0.384496)      venison (0.377063)   \n",
       "10       health        bargain (0.387817)  acquiescing (0.365175)   \n",
       "11  legislative          where (0.365921)          die (0.345302)   \n",
       "12   federative         vapulo (0.379744)          led (0.369526)   \n",
       "13  prerogative          week, (0.410894)       French (0.399346)   \n",
       "14     paternal      Secondly, (0.391736)      mockery (0.375049)   \n",
       "15    political          gross (0.366048)         204. (0.356294)   \n",
       "16        power         power: (0.378586)       stock; (0.375537)   \n",
       "17       tyrant   prescription (0.366177)      soever, (0.364885)   \n",
       "18         king     England’s; (0.357466)   violences: (0.344591)   \n",
       "19  dissolution      violating (0.391697)        forms (0.384087)   \n",
       "\n",
       "             Trained Top 3          Trained Top 4            Trained Top 5  \\\n",
       "0     Gutenberg (0.453032)    attached (0.411804)    Foundation (0.380015)   \n",
       "1     flattered (0.380136)     chanced (0.375924)          190. (0.372523)   \n",
       "2       direct; (0.345035)        writ (0.344350)     favourite (0.339717)   \n",
       "3        1.F.1. (0.339296)       weal, (0.333891)          pure (0.313938)   \n",
       "4          law: (0.365230)    another: (0.356500)    continued, (0.348488)   \n",
       "5      agreeing (0.381267)  consenting (0.364311)      stickler (0.363374)   \n",
       "6        latter (0.360880)     throne; (0.358619)     Almighty: (0.349587)   \n",
       "7     indemnify (0.403222)     Archive (0.401090)          162. (0.388566)   \n",
       "8      devised; (0.365077)    villany. (0.361852)      nourish, (0.358377)   \n",
       "9       careful (0.372775)         Cum (0.355754)     righteous (0.355308)   \n",
       "10    consists, (0.349921)  authority: (0.346687)           12. (0.344153)   \n",
       "11    duration, (0.342903)    freedom, (0.341778)     exception (0.335117)   \n",
       "12    majority: (0.351617)       untie (0.348730)      maturity (0.337431)   \n",
       "13  omnipotency (0.393160)   accounted (0.386962)           age (0.376078)   \n",
       "14       king’s (0.353551)        214. (0.351812)          ruin (0.344638)   \n",
       "15          in: (0.353697)     needed. (0.338409)    miserable, (0.336214)   \n",
       "16       patron (0.343386)  privilege, (0.334428)       excuse, (0.333702)   \n",
       "17    guardians (0.355106)    subject. (0.352139)  consumption, (0.338955)   \n",
       "18    debating, (0.335364)    Children (0.332990)         David (0.332505)   \n",
       "19      called, (0.371473)    practice (0.370252)     variable, (0.369813)   \n",
       "\n",
       "             Untrained Top 1           Untrained Top 2        Untrained Top 3  \\\n",
       "0   ministerially (0.356555)          Chap. (0.330462)     wrought (0.324288)   \n",
       "1       deference (0.338117)        appears (0.330488)      These, (0.322855)   \n",
       "2      otherwise, (0.351606)        thought (0.349163)         on. (0.328751)   \n",
       "3    inclinations (0.410378)        eBooks. (0.338991)        164. (0.333776)   \n",
       "4      dangerous, (0.388810)       conjugal (0.376794)      yellow (0.359922)   \n",
       "5      wilderness (0.375820)  comprehending (0.357659)  children’s (0.343036)   \n",
       "6        slavery, (0.325534)     sacredness (0.317318)        (any (0.317094)   \n",
       "7       security: (0.345496)        Justin, (0.342210)     finding (0.332893)   \n",
       "8    Self-defence (0.402027)       returned (0.372312)      ferro, (0.372173)   \n",
       "9       potestate (0.386430)       settled, (0.308138)     offices (0.306163)   \n",
       "10     dissolved, (0.358068)       passion. (0.334100)    follows: (0.333218)   \n",
       "11        housing (0.350448)         cured. (0.332248)        vero (0.327208)   \n",
       "12        prevent (0.345520)            57. (0.339217)   describes (0.334718)   \n",
       "13        rebels: (0.478924)          pence (0.348867)      appear (0.337215)   \n",
       "14        advance (0.347834)        system. (0.326646)          ac (0.323806)   \n",
       "15     thoroughly (0.389222)        suprema (0.342478)   paternal, (0.321268)   \n",
       "16      tributum, (0.336056)    intolerable (0.323507)         (By (0.321822)   \n",
       "17        500_l_. (0.352819)         cause. (0.342023)           2 (0.335136)   \n",
       "18          since (0.377849)          ages, (0.370721)     senatui (0.352447)   \n",
       "19       commons: (0.356367)  deliberation. (0.328566)       value (0.315360)   \n",
       "\n",
       "            Untrained Top 4        Untrained Top 5  \n",
       "0          find, (0.323200)    pleases, (0.319544)  \n",
       "1       company, (0.321896)       odium (0.319615)  \n",
       "2      violates, (0.326966)     inroads (0.326171)  \n",
       "3     agreement, (0.333652)       unity (0.330755)  \n",
       "4          left; (0.353840)        But, (0.330724)  \n",
       "5         choose (0.342096)     winter, (0.334080)  \n",
       "6           said (0.314947)     defends (0.307193)  \n",
       "7   uncultivated (0.325989)   miserable (0.317754)  \n",
       "8         comes, (0.353006)      honest (0.327401)  \n",
       "9           222. (0.305195)    secretly (0.300081)  \n",
       "10         lived (0.326381)   convoking (0.321636)  \n",
       "11        admits (0.322830)          PG (0.316128)  \n",
       "12        ashes, (0.333056)        eat; (0.332731)  \n",
       "13   convenient, (0.335632)     heaven; (0.330686)  \n",
       "14           per (0.316942)      1.F.4. (0.312978)  \n",
       "15       should, (0.318440)  usefulness (0.311658)  \n",
       "16   propulsandi (0.319499)  succession (0.310854)  \n",
       "17    injustice, (0.314643)       place (0.313433)  \n",
       "18    other-wise (0.347251)      acres, (0.325989)  \n",
       "19     language, (0.311462)      sword, (0.309430)  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_words = [\"Project\", \"slavery\", \"property\", \"war\", \"state\", \"love\", \"land\", \"owner\", \"child\", \"history\", \"health\",\n",
    "                \"legislative\", \"federative\", \"prerogative\", \"paternal\", \"political\", \"power\", \"tyrant\", \"king\", \"dissolution\"]\n",
    "# Compare trained and untrained embeddings\n",
    "comparison_df = compare_trained_untrained(\n",
    "    embeddings_trained=skip_weights_from_class,\n",
    "    embeddings_untrained=skip_model_untrained.embeddings.weight.data,\n",
    "    vocab=vocab,\n",
    "    target_words=target_words,\n",
    "    topn=5\n",
    ")\n",
    "\n",
    "# Display the comparison DataFrame\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained Skip-gram model produces much more meaningful and contextually relevant word embeddings compared to the untrained model:\n",
    "\n",
    "**Trained Model**: The closest words are much more semantically related to the target word. For example, for the word \"Project,\" the top 5 closest words in the trained model include \"Gutenberg,\" \"group,\" and \"Foundation,\" all of which are meaningful associations with projects or collaborative work. Similarly, for the word \"love,\" the closest words are contextually relevant, like \"determination,\" \"beware,\" and \"agreeing,\" which are related to feelings and actions associated with love.\n",
    "\n",
    "**Untrained Model**: The closest words here are much more random and lack meaningful semantic connections. For \"Project,\" the untrained model suggests words like \"ministerially,\" \"Chap.,\" and \"wrought,\" which don't align well with the intended meaning of \"Project.\" Similarly, for \"slavery,\" the untrained model suggests words like \"deference\" and \"appears,\" which are far less contextually relevant than the trained model's suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIX6eMg9zp8A"
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N036CnSl0PaV"
   },
   "source": [
    "Sentence tokenization and word tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3krMpwGOrAYf",
    "outputId": "25acad87-b20d-4b5c-ab5e-143fa85cdcd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 1460\n",
      "Example sentence tokens: ['project', 'gutenberg', 'ebook', 'of', 'second', 'treatise', 'of', 'government', 'this', 'ebook']\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.gutenberg.org/cache/epub/7370/pg7370.txt\"\n",
    "text = requests.get(url).text.lower()\n",
    "\n",
    "sentences = []\n",
    "for sent in sent_tokenize(text):\n",
    "    tokens = word_tokenize(sent)\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    if tokens:\n",
    "        sentences.append(tokens)\n",
    "\n",
    "print(f\"Total sentences: {len(sentences)}\")\n",
    "print(f\"Example sentence tokens: {sentences[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "RcCT5bjfyQj8",
    "outputId": "b3fcc602-81ca-45ff-bc83-dea285992030"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used by DF 11812\n",
      "Read rows: 1460, columns: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"book_df\",\n  \"rows\": 1460,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1158,\n        \"samples\": [\n          \"people are not so easily got out of their old forms as some are apt to suggest\",\n          \"adam was created a perfect man his body and mind in full possession of their strength and reason and so was capable from the first instant of his being to provide for his own support and preservation and govern his actions according to the dictates of the law of reason which god had implanted in him\",\n          \"the liberty of man in society is to be under no other legislative power but that established by consent in the commonwealth nor under the dominion of any will or restraint of any law but what that legislative shall enact according to the trust put in it\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "book_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-887b15bd-f7ee-46db-9221-f0c0f84c1bf1\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>project gutenberg ebook of second treatise of government this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restricti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you may copy it give it away or it under the terms of the project gutenberg license included with this ebook or online at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if you are not located in the united states you will have to check the laws of the country where you are located before using this ebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>title second treatise of government author john locke release date january ebook most recently updated december language english credits dave gowan and chuck greif start of the project gutenberg e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>john locke s second treatise of government was published in the complete unabridged text has been republished several times in edited commentaries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-887b15bd-f7ee-46db-9221-f0c0f84c1bf1')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-887b15bd-f7ee-46db-9221-f0c0f84c1bf1 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-887b15bd-f7ee-46db-9221-f0c0f84c1bf1');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-2283c2b4-ec3f-4bb8-87b1-e59e9e696bfc\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2283c2b4-ec3f-4bb8-87b1-e59e9e696bfc')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-2283c2b4-ec3f-4bb8-87b1-e59e9e696bfc button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                                                                                                                                                                      text\n",
       "0  project gutenberg ebook of second treatise of government this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restricti...\n",
       "1                                                                                you may copy it give it away or it under the terms of the project gutenberg license included with this ebook or online at\n",
       "2                                                                 if you are not located in the united states you will have to check the laws of the country where you are located before using this ebook\n",
       "3  title second treatise of government author john locke release date january ebook most recently updated december language english credits dave gowan and chuck greif start of the project gutenberg e...\n",
       "4                                                       john locke s second treatise of government was published in the complete unabridged text has been republished several times in edited commentaries"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_texts = [\" \".join(tokens) for tokens in sentences]\n",
    "book_df = pd.DataFrame(sentence_texts, columns=[\"text\"])\n",
    "\n",
    "# Memory usage and shape\n",
    "print(f\"Memory used by DF {book_df.memory_usage().sum()}\")\n",
    "print(f\"Read rows: {book_df.shape[0]}, columns: {book_df.shape[1]}\")\n",
    "book_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "fyDNKnmHMj-a",
    "outputId": "1ea75cc3-1b67-4a0b-b645-484b80470a79"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"book_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"project ebooks are often created from several printed editions all of which are confirmed as not protected by copyright in the unless a copyright notice is included\",\n          \"this website includes information about project including how to make donations to the project gutenberg literary archive foundation how to help produce our new ebooks and how to subscribe to our email newsletter to hear about new ebooks\",\n          \"thus we do not necessarily keep ebooks in compliance with any particular paper edition\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-6bb6bbcb-774d-4e08-bffd-044e061fed6d\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>for forty years he produced and distributed project ebooks with only a loose network of volunteer support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>project ebooks are often created from several printed editions all of which are confirmed as not protected by copyright in the unless a copyright notice is included</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>thus we do not necessarily keep ebooks in compliance with any particular paper edition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>most people start at our website which has the main pg search facility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>this website includes information about project including how to make donations to the project gutenberg literary archive foundation how to help produce our new ebooks and how to subscribe to our ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6bb6bbcb-774d-4e08-bffd-044e061fed6d')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-6bb6bbcb-774d-4e08-bffd-044e061fed6d button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-6bb6bbcb-774d-4e08-bffd-044e061fed6d');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-caf8662c-2aad-42e9-ac54-5ce6a80fc02c\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-caf8662c-2aad-42e9-ac54-5ce6a80fc02c')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-caf8662c-2aad-42e9-ac54-5ce6a80fc02c button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                                                                                                                                                                         text\n",
       "1455                                                                                                for forty years he produced and distributed project ebooks with only a loose network of volunteer support\n",
       "1456                                     project ebooks are often created from several printed editions all of which are confirmed as not protected by copyright in the unless a copyright notice is included\n",
       "1457                                                                                                                   thus we do not necessarily keep ebooks in compliance with any particular paper edition\n",
       "1458                                                                                                                                   most people start at our website which has the main pg search facility\n",
       "1459  this website includes information about project including how to make donations to the project gutenberg literary archive foundation how to help produce our new ebooks and how to subscribe to our ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1D7xoO60Upa"
   },
   "source": [
    "Clean-up text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "HcyKLQ_wzjF9",
    "outputId": "f6f275d4-fd45-4ba3-f3db-16647f2babff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 1 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"book_df\",\n  \"rows\": 1460,\n  \"fields\": [\n    {\n      \"column\": \"clean_sentences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1157,\n        \"samples\": [\n          \"people easily got old forms apt suggest\",\n          \"adam created perfect man body mind full possession strength reason capable first instant provide support preservation govern actions according dictates law reason god implanted\",\n          \"liberty man society legislative power established consent commonwealth dominion restraint law legislative shall enact according trust put\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "book_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-1a4f3bd6-4041-4d1c-ab3f-c56c27ed99b1\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>project gutenberg ebook second treatise government ebook use anyone anywhere united states parts world cost almost restrictions whatsoever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>may copy give away terms project gutenberg license included ebook online</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>located united states check laws country located using ebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>title second treatise government author john locke release date january ebook recently updated december language english credits dave gowan chuck greif start project gutenberg ebook second treatis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>john locke second treatise government published complete unabridged text republished several times edited commentaries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a4f3bd6-4041-4d1c-ab3f-c56c27ed99b1')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-1a4f3bd6-4041-4d1c-ab3f-c56c27ed99b1 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-1a4f3bd6-4041-4d1c-ab3f-c56c27ed99b1');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-53679f38-5edf-4952-82a5-a505677f90a6\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-53679f38-5edf-4952-82a5-a505677f90a6')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-53679f38-5edf-4952-82a5-a505677f90a6 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                                                                                                                                                           clean_sentences\n",
       "0                                                               project gutenberg ebook second treatise government ebook use anyone anywhere united states parts world cost almost restrictions whatsoever\n",
       "1                                                                                                                                 may copy give away terms project gutenberg license included ebook online\n",
       "2                                                                                                                                             located united states check laws country located using ebook\n",
       "3  title second treatise government author john locke release date january ebook recently updated december language english credits dave gowan chuck greif start project gutenberg ebook second treatis...\n",
       "4                                                                                   john locke second treatise government published complete unabridged text republished several times edited commentaries"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandarallel.initialize()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "book_df['clean_sentences'] = book_df['text'].parallel_apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "book_df['clean_sentences'] = book_df['clean_sentences'].parallel_apply(lambda x: re.sub('[^a-zA-Z0-9 @ . , : - _]', '', x))\n",
    "\n",
    "book_df = book_df[['clean_sentences']]\n",
    "book_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ski7cRZe0eCM",
    "outputId": "46b469ef-263e-4547-ef4f-b6de5f044e8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['project',\n",
       "  'gutenberg',\n",
       "  'ebook',\n",
       "  'second',\n",
       "  'treatise',\n",
       "  'government',\n",
       "  'ebook',\n",
       "  'use',\n",
       "  'anyone',\n",
       "  'anywhere',\n",
       "  'united',\n",
       "  'states',\n",
       "  'parts',\n",
       "  'world',\n",
       "  'cost',\n",
       "  'almost',\n",
       "  'restrictions',\n",
       "  'whatsoever'],\n",
       " ['may',\n",
       "  'copy',\n",
       "  'give',\n",
       "  'away',\n",
       "  'terms',\n",
       "  'project',\n",
       "  'gutenberg',\n",
       "  'license',\n",
       "  'included',\n",
       "  'ebook',\n",
       "  'online']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [row.split() for row in book_df['clean_sentences']]\n",
    "sentences[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Atqx40xi1xM_"
   },
   "source": [
    "## Create the training examples and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5QdIZog4Cog"
   },
   "source": [
    "Manual training pair generation:\n",
    "\n",
    "The following code manually creates Skip-gram training pairs (center word, context word) using a sliding window. While this step is not required when using `gensim.Word2Vec`, we include it here to demonstrate how training examples and labels are constructed in Skip-gram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VURrwPg516NY"
   },
   "source": [
    "Build vocabulary and index mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mf_gOGbq08QT",
    "outputId": "adc38fa6-e575-4318-9dbe-60aec698e74c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 1028\n"
     ]
    }
   ],
   "source": [
    "# Flatten all words and count\n",
    "word_counts = Counter(word for sentence in sentences for word in sentence)\n",
    "\n",
    "# Optional: filter rare words (min frequency = 5)\n",
    "min_freq = 5\n",
    "vocab = [word for word, count in word_counts.items() if count >= min_freq]\n",
    "\n",
    "# Create mappings\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4PATbPh2MSV"
   },
   "source": [
    "Create skip-gram training pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PnqgQJ9N2Kde",
    "outputId": "568b9935-303f-419c-d033-5ece37dd4f87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training pairs: 74194\n",
      "Example: [(0, 1), (0, 2), (1, 0), (1, 2), (1, 3)]\n",
      "Pair 1: (project, gutenberg)\n",
      "Pair 2: (project, ebook)\n",
      "Pair 3: (gutenberg, project)\n",
      "Pair 4: (gutenberg, ebook)\n",
      "Pair 5: (gutenberg, second)\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "training_pairs = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = [word for word in sentence if word in word2idx]  # filter unknown words\n",
    "    for idx, center_word in enumerate(sentence):\n",
    "        center_idx = word2idx[center_word]\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_pos = idx + w\n",
    "            if w != 0 and 0 <= context_pos < len(sentence):\n",
    "                context_word = sentence[context_pos]\n",
    "                context_idx = word2idx[context_word]\n",
    "                training_pairs.append((center_idx, context_idx))\n",
    "\n",
    "print(f\"Total training pairs: {len(training_pairs)}\")\n",
    "print(\"Example:\", training_pairs[:5])\n",
    "\n",
    "# Convert first 5 training pairs back to words\n",
    "for i in range(5):\n",
    "    center_idx, context_idx = training_pairs[i]\n",
    "    center_word = idx2word[center_idx]\n",
    "    context_word = idx2word[context_idx]\n",
    "    print(f\"Pair {i+1}: ({center_word}, {context_word})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1B9j9S_2exg"
   },
   "source": [
    "Convert to tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wnD6s1CE2S3W",
    "outputId": "050b62f8-b5bf-46d3-ad62-88d46ad92bab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (center word indices): tensor([0, 0, 1, 1, 1])\n",
      "y (context word indices): tensor([1, 2, 0, 2, 3])\n",
      "X_words (center): ['project', 'project', 'gutenberg', 'gutenberg', 'gutenberg']\n",
      "y_words (context): ['gutenberg', 'ebook', 'project', 'ebook', 'second']\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([pair[0] for pair in training_pairs])\n",
    "y = torch.tensor([pair[1] for pair in training_pairs])\n",
    "\n",
    "print(\"X (center word indices):\", X[:5])\n",
    "print(\"y (context word indices):\", y[:5])\n",
    "\n",
    "# Convert first 5 tensor pairs to word format, split into X_words and y_words\n",
    "X_words = [idx2word[X[i].item()] for i in range(5)]\n",
    "y_words = [idx2word[y[i].item()] for i in range(5)]\n",
    "\n",
    "print(\"X_words (center):\", X_words)\n",
    "print(\"y_words (context):\", y_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzRIRffp2yTe"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_AqUNMk93PK"
   },
   "source": [
    "Skip-gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZpdgnM0e2gN0",
    "outputId": "1ea0c03d-c060-4813-8c0c-909c355cdbbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 950 ms, sys: 8.5 ms, total: 958 ms\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "workers = num_processors - 1\n",
    "\n",
    "sg_model = gensim.models.Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,\n",
    "    window=2,\n",
    "    min_count=5,\n",
    "    sg=1,                    # 1 = Skip-gram\n",
    "    compute_loss=True,\n",
    "    workers=workers,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiVHAM9L5xuH"
   },
   "source": [
    "Traing loss computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9KmdTi85eE5",
    "outputId": "75f88f0b-6550-455e-a716-f810bea217a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1401080.375\n",
      "CPU times: user 188 µs, sys: 1.02 ms, total: 1.2 ms\n",
      "Wall time: 1.28 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# getting the training loss value\n",
    "sg_training_loss = sg_model.get_latest_training_loss()\n",
    "print(sg_training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKVem9kw-WWR"
   },
   "source": [
    "CBOW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ivBbecPO-HFn",
    "outputId": "3f9e1e14-d097-4dad-ee63-aaa4897b5528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 545 ms, sys: 12.2 ms, total: 557 ms\n",
      "Wall time: 773 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "workers = num_processors - 1\n",
    "\n",
    "cbow_model = gensim.models.Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,\n",
    "    window=2,\n",
    "    min_count=5,\n",
    "    sg=0,                    # 0 = CBOW\n",
    "    compute_loss=True,\n",
    "    workers=workers,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZBLliPXF-ewM",
    "outputId": "cb57de90-99d0-4551-987b-bd0dd69b7e3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564519.375\n",
      "CPU times: user 410 µs, sys: 0 ns, total: 410 µs\n",
      "Wall time: 367 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# getting the training loss value\n",
    "cbow_training_loss = cbow_model.get_latest_training_loss()\n",
    "print(cbow_training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6jezNDU57D5"
   },
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAM4VMHe5-nC"
   },
   "source": [
    "Compare a few words to evaluate how well the model learned word representations (are they better than random?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6q5XWP3jNOWT"
   },
   "outputs": [],
   "source": [
    "def sg_cbow_similar_words_df(sim_model_sg, sim_model_cbow, word_list, topn=5):\n",
    "    rows = []\n",
    "\n",
    "    for word in word_list:\n",
    "        row = [word]\n",
    "\n",
    "        # Get Skip-gram results\n",
    "        try:\n",
    "            sg_sim = sim_model_sg.wv.most_similar(word, topn=topn)\n",
    "            row.extend([f\"{w} ({sim:.6f})\" for w, sim in sg_sim])\n",
    "        except KeyError:\n",
    "            row.extend([None] * topn)\n",
    "\n",
    "        # Get CBOW results\n",
    "        try:\n",
    "            cbow_sim = sim_model_cbow.wv.most_similar(word, topn=topn)\n",
    "            row.extend([f\"{w} ({sim:.6f})\" for w, sim in cbow_sim])\n",
    "        except KeyError:\n",
    "            row.extend([None] * topn)\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    columns = (\n",
    "        [\"Target Word\"] +\n",
    "        [f\"SG Top{i+1}\" for i in range(topn)] +\n",
    "        [f\"CBOW Top{i+1}\" for i in range(topn)]\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame(rows, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "h9NwKco550yS"
   },
   "outputs": [],
   "source": [
    "target_words = [\"government\", \"slavery\", \"property\", \"war\", \"state\", \"love\", \"land\", \"owner\", \"child\", \"history\", \"health\",\n",
    "                \"legislative\", \"federative\", \"prerogative\", \"paternal\", \"political\", \"power\", \"tyrant\", \"king\", \"dissolution\"]\n",
    "topn = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Cc8D8DpiA066",
    "outputId": "4d461fe4-9cd2-48d5-f5e9-4937486dbca5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"sg_cbow_similar_words_df(sg_model, cbow_model, target_words, topn)\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"Target Word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"government\",\n          \"tyrant\",\n          \"political\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SG Top1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"ends (0.993854)\",\n          \"hundred (0.998007)\",\n          \"member (0.996325)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SG Top2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"members (0.993708)\",\n          \"feed (0.997790)\",\n          \"subjected (0.995950)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SG Top3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"forms (0.993450)\",\n          \"degree (0.997701)\",\n          \"quit (0.995865)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SG Top4\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"societies (0.993133)\",\n          \"revenge (0.997653)\",\n          \"entered (0.995786)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SG Top5\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"secure (0.993059)\",\n          \"reverence (0.997649)\",\n          \"independent (0.995770)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CBOW Top1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"upon (0.999460)\",\n          \"way (0.997695)\",\n          \"king (0.999148)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CBOW Top2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"set (0.999302)\",\n          \"present (0.997542)\",\n          \"governments (0.999112)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CBOW Top3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 19,\n        \"samples\": [\n          \"together (0.999293)\",\n          \"whose (0.998700)\",\n          \"since (0.997636)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CBOW Top4\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"till (0.999262)\",\n          \"possessions (0.997477)\",\n          \"together (0.999076)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CBOW Top5\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"society (0.999259)\",\n          \"long (0.997454)\",\n          \"due (0.999051)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-012d0cd2-9b40-4c7c-979b-8f25395b19e0\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target Word</th>\n",
       "      <th>SG Top1</th>\n",
       "      <th>SG Top2</th>\n",
       "      <th>SG Top3</th>\n",
       "      <th>SG Top4</th>\n",
       "      <th>SG Top5</th>\n",
       "      <th>CBOW Top1</th>\n",
       "      <th>CBOW Top2</th>\n",
       "      <th>CBOW Top3</th>\n",
       "      <th>CBOW Top4</th>\n",
       "      <th>CBOW Top5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>government</td>\n",
       "      <td>ends (0.993854)</td>\n",
       "      <td>members (0.993708)</td>\n",
       "      <td>forms (0.993450)</td>\n",
       "      <td>societies (0.993133)</td>\n",
       "      <td>secure (0.993059)</td>\n",
       "      <td>upon (0.999460)</td>\n",
       "      <td>set (0.999302)</td>\n",
       "      <td>together (0.999293)</td>\n",
       "      <td>till (0.999262)</td>\n",
       "      <td>society (0.999259)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>slavery</td>\n",
       "      <td>pay (0.998278)</td>\n",
       "      <td>continued (0.998122)</td>\n",
       "      <td>representatives (0.998082)</td>\n",
       "      <td>former (0.998056)</td>\n",
       "      <td>food (0.998037)</td>\n",
       "      <td>bound (0.998777)</td>\n",
       "      <td>set (0.998758)</td>\n",
       "      <td>able (0.998742)</td>\n",
       "      <td>agreement (0.998711)</td>\n",
       "      <td>long (0.998634)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>property</td>\n",
       "      <td>become (0.995551)</td>\n",
       "      <td>pleases (0.995544)</td>\n",
       "      <td>human (0.995401)</td>\n",
       "      <td>restraint (0.995378)</td>\n",
       "      <td>members (0.995198)</td>\n",
       "      <td>left (0.999322)</td>\n",
       "      <td>upon (0.999304)</td>\n",
       "      <td>thereby (0.999291)</td>\n",
       "      <td>together (0.999286)</td>\n",
       "      <td>even (0.999285)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>war</td>\n",
       "      <td>puts (0.984027)</td>\n",
       "      <td>man (0.977977)</td>\n",
       "      <td>still (0.977045)</td>\n",
       "      <td>though (0.976346)</td>\n",
       "      <td>common (0.976180)</td>\n",
       "      <td>still (0.998885)</td>\n",
       "      <td>people (0.998856)</td>\n",
       "      <td>though (0.998792)</td>\n",
       "      <td>mankind (0.998736)</td>\n",
       "      <td>private (0.998731)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>state</td>\n",
       "      <td>law (0.973860)</td>\n",
       "      <td>puts (0.960767)</td>\n",
       "      <td>man (0.959877)</td>\n",
       "      <td>liberty (0.956544)</td>\n",
       "      <td>war (0.955352)</td>\n",
       "      <td>law (0.998331)</td>\n",
       "      <td>man (0.997872)</td>\n",
       "      <td>makes (0.997748)</td>\n",
       "      <td>made (0.997732)</td>\n",
       "      <td>king (0.997712)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>love</td>\n",
       "      <td>notwithstanding (0.998241)</td>\n",
       "      <td>hinder (0.998150)</td>\n",
       "      <td>last (0.998102)</td>\n",
       "      <td>commission (0.998042)</td>\n",
       "      <td>doctrine (0.998034)</td>\n",
       "      <td>makes (0.998787)</td>\n",
       "      <td>great (0.998703)</td>\n",
       "      <td>whose (0.998700)</td>\n",
       "      <td>certain (0.998699)</td>\n",
       "      <td>far (0.998697)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>land</td>\n",
       "      <td>conditions (0.993983)</td>\n",
       "      <td>ground (0.993326)</td>\n",
       "      <td>supposing (0.993027)</td>\n",
       "      <td>inhabitants (0.992895)</td>\n",
       "      <td>subdued (0.992872)</td>\n",
       "      <td>made (0.999334)</td>\n",
       "      <td>yet (0.999317)</td>\n",
       "      <td>together (0.999293)</td>\n",
       "      <td>force (0.999282)</td>\n",
       "      <td>makes (0.999274)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owner</td>\n",
       "      <td>agree (0.997564)</td>\n",
       "      <td>ebooks (0.997431)</td>\n",
       "      <td>volunteers (0.997204)</td>\n",
       "      <td>including (0.997112)</td>\n",
       "      <td>defect (0.997032)</td>\n",
       "      <td>others (0.997341)</td>\n",
       "      <td>possession (0.997324)</td>\n",
       "      <td>set (0.997245)</td>\n",
       "      <td>ends (0.997229)</td>\n",
       "      <td>might (0.997201)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>child</td>\n",
       "      <td>equality (0.997602)</td>\n",
       "      <td>tie (0.997448)</td>\n",
       "      <td>education (0.997336)</td>\n",
       "      <td>useful (0.997278)</td>\n",
       "      <td>understood (0.997271)</td>\n",
       "      <td>either (0.999282)</td>\n",
       "      <td>community (0.999275)</td>\n",
       "      <td>people (0.999261)</td>\n",
       "      <td>find (0.999261)</td>\n",
       "      <td>far (0.999258)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>history</td>\n",
       "      <td>speak (0.997824)</td>\n",
       "      <td>head (0.997790)</td>\n",
       "      <td>slaves (0.997787)</td>\n",
       "      <td>immediately (0.997742)</td>\n",
       "      <td>wholly (0.997720)</td>\n",
       "      <td>beginning (0.999207)</td>\n",
       "      <td>must (0.999155)</td>\n",
       "      <td>condition (0.999132)</td>\n",
       "      <td>yet (0.999116)</td>\n",
       "      <td>thing (0.999105)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>health</td>\n",
       "      <td>strange (0.997932)</td>\n",
       "      <td>freemen (0.997760)</td>\n",
       "      <td>slavery (0.997672)</td>\n",
       "      <td>conclude (0.997669)</td>\n",
       "      <td>occasion (0.997667)</td>\n",
       "      <td>god (0.994856)</td>\n",
       "      <td>sword (0.994833)</td>\n",
       "      <td>natural (0.994808)</td>\n",
       "      <td>according (0.994784)</td>\n",
       "      <td>age (0.994689)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>legislative</td>\n",
       "      <td>supreme (0.994990)</td>\n",
       "      <td>making (0.990957)</td>\n",
       "      <td>commonwealth (0.990565)</td>\n",
       "      <td>hands (0.989961)</td>\n",
       "      <td>executive (0.989923)</td>\n",
       "      <td>force (0.999183)</td>\n",
       "      <td>far (0.999142)</td>\n",
       "      <td>still (0.999119)</td>\n",
       "      <td>commonwealth (0.999112)</td>\n",
       "      <td>either (0.999106)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>federative</td>\n",
       "      <td>got (0.997712)</td>\n",
       "      <td>convoking (0.997706)</td>\n",
       "      <td>forfeiture (0.997621)</td>\n",
       "      <td>discretion (0.997591)</td>\n",
       "      <td>placed (0.997549)</td>\n",
       "      <td>many (0.997719)</td>\n",
       "      <td>true (0.997688)</td>\n",
       "      <td>since (0.997636)</td>\n",
       "      <td>things (0.997627)</td>\n",
       "      <td>force (0.997571)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>prerogative</td>\n",
       "      <td>hurt (0.996590)</td>\n",
       "      <td>exposed (0.996516)</td>\n",
       "      <td>exercised (0.996390)</td>\n",
       "      <td>rulers (0.996310)</td>\n",
       "      <td>belonging (0.996232)</td>\n",
       "      <td>people (0.999259)</td>\n",
       "      <td>force (0.999237)</td>\n",
       "      <td>never (0.999227)</td>\n",
       "      <td>either (0.999214)</td>\n",
       "      <td>governments (0.999212)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>paternal</td>\n",
       "      <td>exposed (0.997382)</td>\n",
       "      <td>virtue (0.997066)</td>\n",
       "      <td>hurt (0.997046)</td>\n",
       "      <td>exercise (0.997005)</td>\n",
       "      <td>beyond (0.996985)</td>\n",
       "      <td>either (0.999251)</td>\n",
       "      <td>made (0.999172)</td>\n",
       "      <td>whilst (0.999172)</td>\n",
       "      <td>several (0.999167)</td>\n",
       "      <td>think (0.999163)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>political</td>\n",
       "      <td>member (0.996325)</td>\n",
       "      <td>subjected (0.995950)</td>\n",
       "      <td>quit (0.995865)</td>\n",
       "      <td>entered (0.995786)</td>\n",
       "      <td>independent (0.995770)</td>\n",
       "      <td>king (0.999148)</td>\n",
       "      <td>governments (0.999112)</td>\n",
       "      <td>far (0.999100)</td>\n",
       "      <td>together (0.999076)</td>\n",
       "      <td>due (0.999051)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>power</td>\n",
       "      <td>executive (0.985929)</td>\n",
       "      <td>supreme (0.983532)</td>\n",
       "      <td>absolute (0.982846)</td>\n",
       "      <td>arbitrary (0.981780)</td>\n",
       "      <td>legislative (0.980143)</td>\n",
       "      <td>always (0.999018)</td>\n",
       "      <td>prince (0.999016)</td>\n",
       "      <td>made (0.999011)</td>\n",
       "      <td>either (0.999009)</td>\n",
       "      <td>commonwealth (0.998990)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tyrant</td>\n",
       "      <td>hundred (0.998007)</td>\n",
       "      <td>feed (0.997790)</td>\n",
       "      <td>degree (0.997701)</td>\n",
       "      <td>revenge (0.997653)</td>\n",
       "      <td>reverence (0.997649)</td>\n",
       "      <td>way (0.997695)</td>\n",
       "      <td>present (0.997542)</td>\n",
       "      <td>hundred (0.997497)</td>\n",
       "      <td>possessions (0.997477)</td>\n",
       "      <td>long (0.997454)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>king</td>\n",
       "      <td>besides (0.996251)</td>\n",
       "      <td>hypothesis (0.996171)</td>\n",
       "      <td>commission (0.995942)</td>\n",
       "      <td>proper (0.995937)</td>\n",
       "      <td>usually (0.995771)</td>\n",
       "      <td>together (0.999465)</td>\n",
       "      <td>must (0.999431)</td>\n",
       "      <td>done (0.999428)</td>\n",
       "      <td>much (0.999411)</td>\n",
       "      <td>age (0.999387)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dissolution</td>\n",
       "      <td>exposed (0.998185)</td>\n",
       "      <td>presently (0.998153)</td>\n",
       "      <td>small (0.998096)</td>\n",
       "      <td>legislators (0.998078)</td>\n",
       "      <td>occasions (0.998065)</td>\n",
       "      <td>presently (0.998247)</td>\n",
       "      <td>prince (0.998175)</td>\n",
       "      <td>think (0.998083)</td>\n",
       "      <td>form (0.998039)</td>\n",
       "      <td>either (0.997989)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-012d0cd2-9b40-4c7c-979b-8f25395b19e0')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-012d0cd2-9b40-4c7c-979b-8f25395b19e0 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-012d0cd2-9b40-4c7c-979b-8f25395b19e0');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-35d31d78-1d20-4a08-9414-902bd3e89eae\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-35d31d78-1d20-4a08-9414-902bd3e89eae')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-35d31d78-1d20-4a08-9414-902bd3e89eae button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "    Target Word                     SG Top1                SG Top2  \\\n",
       "0    government             ends (0.993854)     members (0.993708)   \n",
       "1       slavery              pay (0.998278)   continued (0.998122)   \n",
       "2      property           become (0.995551)     pleases (0.995544)   \n",
       "3           war             puts (0.984027)         man (0.977977)   \n",
       "4         state              law (0.973860)        puts (0.960767)   \n",
       "5          love  notwithstanding (0.998241)      hinder (0.998150)   \n",
       "6          land       conditions (0.993983)      ground (0.993326)   \n",
       "7         owner            agree (0.997564)      ebooks (0.997431)   \n",
       "8         child         equality (0.997602)         tie (0.997448)   \n",
       "9       history            speak (0.997824)        head (0.997790)   \n",
       "10       health          strange (0.997932)     freemen (0.997760)   \n",
       "11  legislative          supreme (0.994990)      making (0.990957)   \n",
       "12   federative              got (0.997712)   convoking (0.997706)   \n",
       "13  prerogative             hurt (0.996590)     exposed (0.996516)   \n",
       "14     paternal          exposed (0.997382)      virtue (0.997066)   \n",
       "15    political           member (0.996325)   subjected (0.995950)   \n",
       "16        power        executive (0.985929)     supreme (0.983532)   \n",
       "17       tyrant          hundred (0.998007)        feed (0.997790)   \n",
       "18         king          besides (0.996251)  hypothesis (0.996171)   \n",
       "19  dissolution          exposed (0.998185)   presently (0.998153)   \n",
       "\n",
       "                       SG Top3                 SG Top4  \\\n",
       "0             forms (0.993450)    societies (0.993133)   \n",
       "1   representatives (0.998082)       former (0.998056)   \n",
       "2             human (0.995401)    restraint (0.995378)   \n",
       "3             still (0.977045)       though (0.976346)   \n",
       "4               man (0.959877)      liberty (0.956544)   \n",
       "5              last (0.998102)   commission (0.998042)   \n",
       "6         supposing (0.993027)  inhabitants (0.992895)   \n",
       "7        volunteers (0.997204)    including (0.997112)   \n",
       "8         education (0.997336)       useful (0.997278)   \n",
       "9            slaves (0.997787)  immediately (0.997742)   \n",
       "10          slavery (0.997672)     conclude (0.997669)   \n",
       "11     commonwealth (0.990565)        hands (0.989961)   \n",
       "12       forfeiture (0.997621)   discretion (0.997591)   \n",
       "13        exercised (0.996390)       rulers (0.996310)   \n",
       "14             hurt (0.997046)     exercise (0.997005)   \n",
       "15             quit (0.995865)      entered (0.995786)   \n",
       "16         absolute (0.982846)    arbitrary (0.981780)   \n",
       "17           degree (0.997701)      revenge (0.997653)   \n",
       "18       commission (0.995942)       proper (0.995937)   \n",
       "19            small (0.998096)  legislators (0.998078)   \n",
       "\n",
       "                   SG Top5             CBOW Top1               CBOW Top2  \\\n",
       "0        secure (0.993059)       upon (0.999460)          set (0.999302)   \n",
       "1          food (0.998037)      bound (0.998777)          set (0.998758)   \n",
       "2       members (0.995198)       left (0.999322)         upon (0.999304)   \n",
       "3        common (0.976180)      still (0.998885)       people (0.998856)   \n",
       "4           war (0.955352)        law (0.998331)          man (0.997872)   \n",
       "5      doctrine (0.998034)      makes (0.998787)        great (0.998703)   \n",
       "6       subdued (0.992872)       made (0.999334)          yet (0.999317)   \n",
       "7        defect (0.997032)     others (0.997341)   possession (0.997324)   \n",
       "8    understood (0.997271)     either (0.999282)    community (0.999275)   \n",
       "9        wholly (0.997720)  beginning (0.999207)         must (0.999155)   \n",
       "10     occasion (0.997667)        god (0.994856)        sword (0.994833)   \n",
       "11    executive (0.989923)      force (0.999183)          far (0.999142)   \n",
       "12       placed (0.997549)       many (0.997719)         true (0.997688)   \n",
       "13    belonging (0.996232)     people (0.999259)        force (0.999237)   \n",
       "14       beyond (0.996985)     either (0.999251)         made (0.999172)   \n",
       "15  independent (0.995770)       king (0.999148)  governments (0.999112)   \n",
       "16  legislative (0.980143)     always (0.999018)       prince (0.999016)   \n",
       "17    reverence (0.997649)        way (0.997695)      present (0.997542)   \n",
       "18      usually (0.995771)   together (0.999465)         must (0.999431)   \n",
       "19    occasions (0.998065)  presently (0.998247)       prince (0.998175)   \n",
       "\n",
       "               CBOW Top3                CBOW Top4                CBOW Top5  \n",
       "0    together (0.999293)          till (0.999262)       society (0.999259)  \n",
       "1        able (0.998742)     agreement (0.998711)          long (0.998634)  \n",
       "2     thereby (0.999291)      together (0.999286)          even (0.999285)  \n",
       "3      though (0.998792)       mankind (0.998736)       private (0.998731)  \n",
       "4       makes (0.997748)          made (0.997732)          king (0.997712)  \n",
       "5       whose (0.998700)       certain (0.998699)           far (0.998697)  \n",
       "6    together (0.999293)         force (0.999282)         makes (0.999274)  \n",
       "7         set (0.997245)          ends (0.997229)         might (0.997201)  \n",
       "8      people (0.999261)          find (0.999261)           far (0.999258)  \n",
       "9   condition (0.999132)           yet (0.999116)         thing (0.999105)  \n",
       "10    natural (0.994808)     according (0.994784)           age (0.994689)  \n",
       "11      still (0.999119)  commonwealth (0.999112)        either (0.999106)  \n",
       "12      since (0.997636)        things (0.997627)         force (0.997571)  \n",
       "13      never (0.999227)        either (0.999214)   governments (0.999212)  \n",
       "14     whilst (0.999172)       several (0.999167)         think (0.999163)  \n",
       "15        far (0.999100)      together (0.999076)           due (0.999051)  \n",
       "16       made (0.999011)        either (0.999009)  commonwealth (0.998990)  \n",
       "17    hundred (0.997497)   possessions (0.997477)          long (0.997454)  \n",
       "18       done (0.999428)          much (0.999411)           age (0.999387)  \n",
       "19      think (0.998083)          form (0.998039)        either (0.997989)  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_cbow_similar_words_df(sg_model, cbow_model, target_words, topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cj907mIWBRHC",
    "outputId": "b4d6a15d-a0d0-4445-ffb1-791aaf71648c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random words from corpus: ['facto', 'information', 'remedy', 'since', 'consequently', 'guilty', 'wise', 'fruits', 'government', 'grown', 'change', 'depending', 'magistrates', 'works', 'apt', 'greatest', 'world', 'easy', 'resisting', 'employed']\n"
     ]
    }
   ],
   "source": [
    "vocab_words = list(sg_model.wv.index_to_key)\n",
    "random_words = random.sample(vocab_words, 20)\n",
    "print(\"Random words from corpus:\", random_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cc9xuBtdBtk1",
    "outputId": "0273c68b-1029-4463-9d1b-5c239fb9463d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"sg_cbow_similar_words_df(sg_model, cbow_model, random_words, topn)\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"Target Word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"facto\",\n          \"easy\",\n          \"greatest\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SG Top1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"belongs (0.997854)\",\n          \"infancy (0.998044)\",\n          \"small (0.997778)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SG Top2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"words (0.997834)\",\n          \"double (0.997961)\",\n          \"dangerous (0.997715)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SG Top3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"obey (0.997764)\",\n          \"direct (0.997865)\",\n          \"speak (0.997670)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SG Top4\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"hence (0.997759)\",\n          \"resisted (0.997800)\",\n          \"injured (0.997622)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SG Top5\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"kings (0.997744)\",\n          \"superiority (0.997791)\",\n          \"food (0.997594)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CBOW Top1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"far (0.997855)\",\n          \"thing (0.998999)\",\n          \"private (0.999115)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CBOW Top2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"great (0.997734)\",\n          \"much (0.998974)\",\n          \"found (0.999110)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CBOW Top3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"still (0.997662)\",\n          \"king (0.998922)\",\n          \"taken (0.999044)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CBOW Top4\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"received (0.997662)\",\n          \"may (0.998898)\",\n          \"done (0.999024)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CBOW Top5\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"hath (0.997652)\",\n          \"great (0.998889)\",\n          \"yet (0.999006)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-8454c1d0-cbdb-4e2f-82f6-17c38e97c6af\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target Word</th>\n",
       "      <th>SG Top1</th>\n",
       "      <th>SG Top2</th>\n",
       "      <th>SG Top3</th>\n",
       "      <th>SG Top4</th>\n",
       "      <th>SG Top5</th>\n",
       "      <th>CBOW Top1</th>\n",
       "      <th>CBOW Top2</th>\n",
       "      <th>CBOW Top3</th>\n",
       "      <th>CBOW Top4</th>\n",
       "      <th>CBOW Top5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>facto</td>\n",
       "      <td>belongs (0.997854)</td>\n",
       "      <td>words (0.997834)</td>\n",
       "      <td>obey (0.997764)</td>\n",
       "      <td>hence (0.997759)</td>\n",
       "      <td>kings (0.997744)</td>\n",
       "      <td>far (0.997855)</td>\n",
       "      <td>great (0.997734)</td>\n",
       "      <td>still (0.997662)</td>\n",
       "      <td>received (0.997662)</td>\n",
       "      <td>hath (0.997652)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>information</td>\n",
       "      <td>paragraph (0.997715)</td>\n",
       "      <td>copies (0.997364)</td>\n",
       "      <td>ebook (0.997334)</td>\n",
       "      <td>distribution (0.997250)</td>\n",
       "      <td>donations (0.996666)</td>\n",
       "      <td>foundation (0.998346)</td>\n",
       "      <td>title (0.998315)</td>\n",
       "      <td>people (0.998300)</td>\n",
       "      <td>king (0.998295)</td>\n",
       "      <td>whilst (0.998264)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>remedy</td>\n",
       "      <td>open (0.997710)</td>\n",
       "      <td>mean (0.997468)</td>\n",
       "      <td>lies (0.997372)</td>\n",
       "      <td>answer (0.997331)</td>\n",
       "      <td>judges (0.997160)</td>\n",
       "      <td>much (0.999154)</td>\n",
       "      <td>country (0.999152)</td>\n",
       "      <td>either (0.999131)</td>\n",
       "      <td>little (0.999114)</td>\n",
       "      <td>ought (0.999106)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>since</td>\n",
       "      <td>keep (0.997601)</td>\n",
       "      <td>advantage (0.997580)</td>\n",
       "      <td>independent (0.997537)</td>\n",
       "      <td>owe (0.997500)</td>\n",
       "      <td>seize (0.997463)</td>\n",
       "      <td>always (0.999280)</td>\n",
       "      <td>first (0.999267)</td>\n",
       "      <td>way (0.999262)</td>\n",
       "      <td>must (0.999248)</td>\n",
       "      <td>people (0.999248)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>consequently</td>\n",
       "      <td>tyranny (0.998103)</td>\n",
       "      <td>danger (0.998099)</td>\n",
       "      <td>doth (0.998076)</td>\n",
       "      <td>seize (0.997997)</td>\n",
       "      <td>kingdom (0.997977)</td>\n",
       "      <td>thing (0.998590)</td>\n",
       "      <td>king (0.998553)</td>\n",
       "      <td>execution (0.998513)</td>\n",
       "      <td>better (0.998470)</td>\n",
       "      <td>must (0.998465)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>guilty</td>\n",
       "      <td>room (0.998304)</td>\n",
       "      <td>legislators (0.998151)</td>\n",
       "      <td>food (0.998130)</td>\n",
       "      <td>remain (0.998115)</td>\n",
       "      <td>join (0.998111)</td>\n",
       "      <td>still (0.998307)</td>\n",
       "      <td>upon (0.998277)</td>\n",
       "      <td>well (0.998265)</td>\n",
       "      <td>answer (0.998262)</td>\n",
       "      <td>till (0.998215)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wise</td>\n",
       "      <td>affairs (0.998067)</td>\n",
       "      <td>monarchies (0.998058)</td>\n",
       "      <td>keep (0.998046)</td>\n",
       "      <td>looked (0.998041)</td>\n",
       "      <td>finds (0.998030)</td>\n",
       "      <td>distinct (0.996497)</td>\n",
       "      <td>members (0.996390)</td>\n",
       "      <td>say (0.996340)</td>\n",
       "      <td>evident (0.996335)</td>\n",
       "      <td>make (0.996312)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fruits</td>\n",
       "      <td>speak (0.998002)</td>\n",
       "      <td>accordingly (0.997996)</td>\n",
       "      <td>last (0.997960)</td>\n",
       "      <td>conjunction (0.997943)</td>\n",
       "      <td>conquered (0.997885)</td>\n",
       "      <td>also (0.997175)</td>\n",
       "      <td>parents (0.997134)</td>\n",
       "      <td>cases (0.996999)</td>\n",
       "      <td>copyright (0.996997)</td>\n",
       "      <td>family (0.996987)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>government</td>\n",
       "      <td>ends (0.993854)</td>\n",
       "      <td>members (0.993708)</td>\n",
       "      <td>forms (0.993450)</td>\n",
       "      <td>societies (0.993133)</td>\n",
       "      <td>secure (0.993059)</td>\n",
       "      <td>upon (0.999460)</td>\n",
       "      <td>set (0.999302)</td>\n",
       "      <td>together (0.999293)</td>\n",
       "      <td>till (0.999262)</td>\n",
       "      <td>society (0.999259)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>grown</td>\n",
       "      <td>commonly (0.997983)</td>\n",
       "      <td>ambition (0.997965)</td>\n",
       "      <td>degree (0.997822)</td>\n",
       "      <td>concerning (0.997820)</td>\n",
       "      <td>danger (0.997795)</td>\n",
       "      <td>form (0.996268)</td>\n",
       "      <td>little (0.996217)</td>\n",
       "      <td>necessary (0.996153)</td>\n",
       "      <td>far (0.996136)</td>\n",
       "      <td>commonly (0.996111)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>change</td>\n",
       "      <td>commission (0.997995)</td>\n",
       "      <td>resisted (0.997993)</td>\n",
       "      <td>strange (0.997989)</td>\n",
       "      <td>usually (0.997981)</td>\n",
       "      <td>lost (0.997966)</td>\n",
       "      <td>people (0.999203)</td>\n",
       "      <td>little (0.999191)</td>\n",
       "      <td>still (0.999166)</td>\n",
       "      <td>well (0.999137)</td>\n",
       "      <td>must (0.999125)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>depending</td>\n",
       "      <td>words (0.998084)</td>\n",
       "      <td>usually (0.998078)</td>\n",
       "      <td>allow (0.998002)</td>\n",
       "      <td>affection (0.997970)</td>\n",
       "      <td>keep (0.997936)</td>\n",
       "      <td>together (0.995251)</td>\n",
       "      <td>words (0.995204)</td>\n",
       "      <td>farther (0.995156)</td>\n",
       "      <td>thing (0.995108)</td>\n",
       "      <td>end (0.995053)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>magistrates</td>\n",
       "      <td>degree (0.998325)</td>\n",
       "      <td>increase (0.998063)</td>\n",
       "      <td>food (0.998040)</td>\n",
       "      <td>lying (0.998018)</td>\n",
       "      <td>legislators (0.997963)</td>\n",
       "      <td>amongst (0.997894)</td>\n",
       "      <td>country (0.997776)</td>\n",
       "      <td>world (0.997751)</td>\n",
       "      <td>force (0.997734)</td>\n",
       "      <td>either (0.997724)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>works</td>\n",
       "      <td>work (0.992205)</td>\n",
       "      <td>electronic (0.988397)</td>\n",
       "      <td>gutenberg (0.980009)</td>\n",
       "      <td>license (0.971887)</td>\n",
       "      <td>literary (0.964281)</td>\n",
       "      <td>work (0.999106)</td>\n",
       "      <td>full (0.998712)</td>\n",
       "      <td>license (0.998687)</td>\n",
       "      <td>foundation (0.998346)</td>\n",
       "      <td>terms (0.998307)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>apt</td>\n",
       "      <td>small (0.998172)</td>\n",
       "      <td>founded (0.998116)</td>\n",
       "      <td>sure (0.998074)</td>\n",
       "      <td>grass (0.998022)</td>\n",
       "      <td>obey (0.998011)</td>\n",
       "      <td>either (0.999253)</td>\n",
       "      <td>always (0.999178)</td>\n",
       "      <td>act (0.999156)</td>\n",
       "      <td>thereby (0.999144)</td>\n",
       "      <td>time (0.999143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>greatest</td>\n",
       "      <td>small (0.997778)</td>\n",
       "      <td>dangerous (0.997715)</td>\n",
       "      <td>speak (0.997670)</td>\n",
       "      <td>injured (0.997622)</td>\n",
       "      <td>food (0.997594)</td>\n",
       "      <td>private (0.999115)</td>\n",
       "      <td>found (0.999110)</td>\n",
       "      <td>taken (0.999044)</td>\n",
       "      <td>done (0.999024)</td>\n",
       "      <td>yet (0.999006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>world</td>\n",
       "      <td>destructive (0.993541)</td>\n",
       "      <td>generally (0.993482)</td>\n",
       "      <td>beginning (0.993468)</td>\n",
       "      <td>succession (0.993445)</td>\n",
       "      <td>altered (0.993277)</td>\n",
       "      <td>king (0.999370)</td>\n",
       "      <td>must (0.999346)</td>\n",
       "      <td>original (0.999334)</td>\n",
       "      <td>thing (0.999319)</td>\n",
       "      <td>long (0.999307)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>easy</td>\n",
       "      <td>infancy (0.998044)</td>\n",
       "      <td>double (0.997961)</td>\n",
       "      <td>direct (0.997865)</td>\n",
       "      <td>resisted (0.997800)</td>\n",
       "      <td>superiority (0.997791)</td>\n",
       "      <td>thing (0.998999)</td>\n",
       "      <td>much (0.998974)</td>\n",
       "      <td>king (0.998922)</td>\n",
       "      <td>may (0.998898)</td>\n",
       "      <td>great (0.998889)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>resisting</td>\n",
       "      <td>food (0.997948)</td>\n",
       "      <td>reverence (0.997934)</td>\n",
       "      <td>whoever (0.997879)</td>\n",
       "      <td>account (0.997853)</td>\n",
       "      <td>secondly (0.997844)</td>\n",
       "      <td>whether (0.996767)</td>\n",
       "      <td>well (0.996732)</td>\n",
       "      <td>beginning (0.996687)</td>\n",
       "      <td>hath (0.996636)</td>\n",
       "      <td>great (0.996617)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>employed</td>\n",
       "      <td>sure (0.998012)</td>\n",
       "      <td>join (0.997887)</td>\n",
       "      <td>leave (0.997874)</td>\n",
       "      <td>advantage (0.997855)</td>\n",
       "      <td>occasions (0.997849)</td>\n",
       "      <td>yet (0.997721)</td>\n",
       "      <td>reason (0.997718)</td>\n",
       "      <td>thereby (0.997593)</td>\n",
       "      <td>perhaps (0.997586)</td>\n",
       "      <td>made (0.997558)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8454c1d0-cbdb-4e2f-82f6-17c38e97c6af')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-8454c1d0-cbdb-4e2f-82f6-17c38e97c6af button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-8454c1d0-cbdb-4e2f-82f6-17c38e97c6af');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-011406b9-0a68-4e80-8345-ffa05dd9d6ce\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-011406b9-0a68-4e80-8345-ffa05dd9d6ce')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-011406b9-0a68-4e80-8345-ffa05dd9d6ce button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "     Target Word                 SG Top1                 SG Top2  \\\n",
       "0          facto      belongs (0.997854)        words (0.997834)   \n",
       "1    information    paragraph (0.997715)       copies (0.997364)   \n",
       "2         remedy         open (0.997710)         mean (0.997468)   \n",
       "3          since         keep (0.997601)    advantage (0.997580)   \n",
       "4   consequently      tyranny (0.998103)       danger (0.998099)   \n",
       "5         guilty         room (0.998304)  legislators (0.998151)   \n",
       "6           wise      affairs (0.998067)   monarchies (0.998058)   \n",
       "7         fruits        speak (0.998002)  accordingly (0.997996)   \n",
       "8     government         ends (0.993854)      members (0.993708)   \n",
       "9          grown     commonly (0.997983)     ambition (0.997965)   \n",
       "10        change   commission (0.997995)     resisted (0.997993)   \n",
       "11     depending        words (0.998084)      usually (0.998078)   \n",
       "12   magistrates       degree (0.998325)     increase (0.998063)   \n",
       "13         works         work (0.992205)   electronic (0.988397)   \n",
       "14           apt        small (0.998172)      founded (0.998116)   \n",
       "15      greatest        small (0.997778)    dangerous (0.997715)   \n",
       "16         world  destructive (0.993541)    generally (0.993482)   \n",
       "17          easy      infancy (0.998044)       double (0.997961)   \n",
       "18     resisting         food (0.997948)    reverence (0.997934)   \n",
       "19      employed         sure (0.998012)         join (0.997887)   \n",
       "\n",
       "                   SG Top3                  SG Top4                 SG Top5  \\\n",
       "0          obey (0.997764)         hence (0.997759)        kings (0.997744)   \n",
       "1         ebook (0.997334)  distribution (0.997250)    donations (0.996666)   \n",
       "2          lies (0.997372)        answer (0.997331)       judges (0.997160)   \n",
       "3   independent (0.997537)           owe (0.997500)        seize (0.997463)   \n",
       "4          doth (0.998076)         seize (0.997997)      kingdom (0.997977)   \n",
       "5          food (0.998130)        remain (0.998115)         join (0.998111)   \n",
       "6          keep (0.998046)        looked (0.998041)        finds (0.998030)   \n",
       "7          last (0.997960)   conjunction (0.997943)    conquered (0.997885)   \n",
       "8         forms (0.993450)     societies (0.993133)       secure (0.993059)   \n",
       "9        degree (0.997822)    concerning (0.997820)       danger (0.997795)   \n",
       "10      strange (0.997989)       usually (0.997981)         lost (0.997966)   \n",
       "11        allow (0.998002)     affection (0.997970)         keep (0.997936)   \n",
       "12         food (0.998040)         lying (0.998018)  legislators (0.997963)   \n",
       "13    gutenberg (0.980009)       license (0.971887)     literary (0.964281)   \n",
       "14         sure (0.998074)         grass (0.998022)         obey (0.998011)   \n",
       "15        speak (0.997670)       injured (0.997622)         food (0.997594)   \n",
       "16    beginning (0.993468)    succession (0.993445)      altered (0.993277)   \n",
       "17       direct (0.997865)      resisted (0.997800)  superiority (0.997791)   \n",
       "18      whoever (0.997879)       account (0.997853)     secondly (0.997844)   \n",
       "19        leave (0.997874)     advantage (0.997855)    occasions (0.997849)   \n",
       "\n",
       "                CBOW Top1           CBOW Top2             CBOW Top3  \\\n",
       "0          far (0.997855)    great (0.997734)      still (0.997662)   \n",
       "1   foundation (0.998346)    title (0.998315)     people (0.998300)   \n",
       "2         much (0.999154)  country (0.999152)     either (0.999131)   \n",
       "3       always (0.999280)    first (0.999267)        way (0.999262)   \n",
       "4        thing (0.998590)     king (0.998553)  execution (0.998513)   \n",
       "5        still (0.998307)     upon (0.998277)       well (0.998265)   \n",
       "6     distinct (0.996497)  members (0.996390)        say (0.996340)   \n",
       "7         also (0.997175)  parents (0.997134)      cases (0.996999)   \n",
       "8         upon (0.999460)      set (0.999302)   together (0.999293)   \n",
       "9         form (0.996268)   little (0.996217)  necessary (0.996153)   \n",
       "10      people (0.999203)   little (0.999191)      still (0.999166)   \n",
       "11    together (0.995251)    words (0.995204)    farther (0.995156)   \n",
       "12     amongst (0.997894)  country (0.997776)      world (0.997751)   \n",
       "13        work (0.999106)     full (0.998712)    license (0.998687)   \n",
       "14      either (0.999253)   always (0.999178)        act (0.999156)   \n",
       "15     private (0.999115)    found (0.999110)      taken (0.999044)   \n",
       "16        king (0.999370)     must (0.999346)   original (0.999334)   \n",
       "17       thing (0.998999)     much (0.998974)       king (0.998922)   \n",
       "18     whether (0.996767)     well (0.996732)  beginning (0.996687)   \n",
       "19         yet (0.997721)   reason (0.997718)    thereby (0.997593)   \n",
       "\n",
       "                CBOW Top4            CBOW Top5  \n",
       "0     received (0.997662)      hath (0.997652)  \n",
       "1         king (0.998295)    whilst (0.998264)  \n",
       "2       little (0.999114)     ought (0.999106)  \n",
       "3         must (0.999248)    people (0.999248)  \n",
       "4       better (0.998470)      must (0.998465)  \n",
       "5       answer (0.998262)      till (0.998215)  \n",
       "6      evident (0.996335)      make (0.996312)  \n",
       "7    copyright (0.996997)    family (0.996987)  \n",
       "8         till (0.999262)   society (0.999259)  \n",
       "9          far (0.996136)  commonly (0.996111)  \n",
       "10        well (0.999137)      must (0.999125)  \n",
       "11       thing (0.995108)       end (0.995053)  \n",
       "12       force (0.997734)    either (0.997724)  \n",
       "13  foundation (0.998346)     terms (0.998307)  \n",
       "14     thereby (0.999144)      time (0.999143)  \n",
       "15        done (0.999024)       yet (0.999006)  \n",
       "16       thing (0.999319)      long (0.999307)  \n",
       "17         may (0.998898)     great (0.998889)  \n",
       "18        hath (0.996636)     great (0.996617)  \n",
       "19     perhaps (0.997586)      made (0.997558)  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_cbow_similar_words_df(sg_model, cbow_model, random_words, topn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scog6IrA9wUr"
   },
   "source": [
    "# Skip-gram Model Evaluation Summary\n",
    "\n",
    "Overall, the Skip-gram model trained on the Gutenberg text shows moderate performance. It performs well on many target words, especially those related to social or political context. For example:\n",
    "\n",
    "- `\"slavery\"` returns similar words like `'pay'`, `'continued'`, and `'representatives'`, which are contextually aligned with the narrative around governance and rights.\n",
    "- `\"history\"` brings up `'speak'`, `'freemen'`, and `'head'`, which are all conceptually relevant.\n",
    "- `\"owner\"` and `\"love\"` also show relatively meaningful top results that reflect relationships or transitions commonly seen in literature.\n",
    "\n",
    "However, there are also cases where the model doesn't return strong semantic matches:\n",
    "\n",
    "- `\"war\"` shows top results like `'puts'`, `'man'`, and `'food'`, which are too generic.\n",
    "- `\"tyrant\"` and `\"dissolution\"` return contextually weak terms such as `'hundred'`, `'presently'`, and `'occasions'`, which lack clear relevance.\n",
    "- `\"prerogative\"` returns verbs like `'hurt'` and `'entered'`, which may be grammatically related but don't reflect deeper meaning.\n",
    "\n",
    "This variation is likely due to the limited size and thematic scope of the training corpus (a single literary book). The context window (`window=2`) and relatively short training time (10 epochs) also restrict how well the model can capture broader or abstract relationships.\n",
    "\n",
    "To improve the model, using a larger and more diverse dataset, increasing the window size, or training longer could help strengthen the word embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRKch7tt_bMb"
   },
   "source": [
    "# CBOW Model Evaluation Summary\n",
    "\n",
    "The CBOW model trained on the same Gutenberg text demonstrates reasonable performance but tends to return more general or syntactically frequent terms. It captures some meaningful relationships, but overall the results are more abstract or less thematically grounded compared to the Skip-gram model.\n",
    "\n",
    "For example:\n",
    "\n",
    "- `\"slavery\"` returns similar words like `'bound'`, `'set'`, and `'agreement'`, which are loosely connected but not as contextually deep as in the Skip-gram model.\n",
    "- `\"history\"` brings up terms like `'beginning'`, `'must'`, and `'condition'`, which are common in narrative structure but less semantically rich.\n",
    "- `\"property\"` and `\"government\"` return words like `'left'`, `'upon'`, and `'set'`, which may reflect common collocations but not deeper meaning.\n",
    "\n",
    "Some results also include broad or ambiguous terms:\n",
    "\n",
    "- `\"tyrant\"` returns words like `'way'`, `'present'`, and `'done'`, which are vague.\n",
    "- `\"dissolution\"` shows words like `'presently'`, `'prince'`, and `'form'`, which lack thematic clarity.\n",
    "- `\"prerogative\"` returns `\"people\"`, `\"force\"`, and `\"governments\"`, which are thematically nearby but quite generic.\n",
    "\n",
    "This is expected, as CBOW averages context words to predict the center word, which can dilute the specificity of the representation. Like Skip-gram, CBOW performance is also affected by the small and domain-specific training corpus.\n",
    "\n",
    "Training on a larger and more diverse dataset or adjusting training parameters (like window size or epochs) would likely improve the quality of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4ILOrzz_jgg"
   },
   "source": [
    "# Skip-gram vs. CBOW Comparison\n",
    "## 1. From the pytorch function we can see that:\n",
    "\n",
    "\n",
    " **Input Handling**\n",
    "   - **CBOW:** In CBOW, the model takes the context words (surrounding words) and tries to predict the target word. The input to the model is a collection of context words (e.g., a sliding window around a target word), and the average of their embeddings is computed to make a prediction for the target word. In the code, the line:\n",
    "     ```python\n",
    "     embeds = self.embeddings(inputs).mean(dim=1)\n",
    "     ```\n",
    "     indicates that the embeddings for the input words are averaged along dimension 1 (which corresponds to the context words). This averaged embedding is then passed to the linear layer for prediction.\n",
    "\n",
    "   - **Skip-gram:** In Skip-gram, the model works in the opposite direction. It takes the target word as input and tries to predict the surrounding context words. The input is a single word (the target word), and its embedding is looked up directly without any averaging. In the code, the line:\n",
    "     ```python\n",
    "     embeds = self.embeddings(inputs)\n",
    "     ```\n",
    "     shows that the embedding for the target word is directly retrieved (not averaged). This embedding is then passed to the linear layer for the prediction of the context words.\n",
    "\n",
    " **Embedding Aggregation**\n",
    "   - **CBOW:** The embeddings of the context words are aggregated (averaged) before passing them through the linear layer. This aggregation step is essential for CBOW to combine information from multiple context words into a single vector that is used to predict the target word.\n",
    "   - **Skip-gram:** In Skip-gram, there is no aggregation. The model uses the embedding of the single target word as is, and directly predicts the context words based on that single embedding.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2. From the gensim's Word2Vec results we can see that:\n",
    "\n",
    "> Skip-gram performs better when capturing meaningful semantic relationships. For instance, it identifies connections between \"slavery\" and words like \"representatives\" and \"continued\", while CBOW returns more generic alternatives like \"bound\" and \"set\". Similarly, Skip-gram's results for \"history\" and \"owner\" are more focused and contextually aligned with the themes in the book.\n",
    "\n",
    "> After randomly selecting 20 target words, the results still show that **Skip-gram** produces more **semantically meaningful** outputs, while **CBOW** tends to return more **syntactically frequent or general-purpose** words.\n",
    "\n",
    "> In terms of training loss, the Skip-gram model had a higher loss value (1,401,080.375) compared to the CBOW model (564,519.375). However, lower loss in CBOW does not necessarily indicate better semantic performance — especially in smaller, domain-specific corpora. Skip-gram typically optimizes more individual word-context predictions (**one-to-many**), which naturally leads to a higher total loss. It also tends to take **longer to train** due to the increased number of training examples and updates per word. On the other hand, CBOW is inherently **faster** because it averages context words to predict a single center word (**many-to-one**), reducing the number of updates. Therefore, when comparing training loss and time, it is important to consider the underlying architecture and how each model handles word frequency, context richness, and training efficiency.\n",
    "\n",
    "In summary, Skip-gram showed stronger performance for this task due to its ability to learn from specific word pairs, which is particularly important when the corpus is small and the domain is narrow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ox1Udut4G5QG"
   },
   "source": [
    "# Further Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVlqd-3_G7Za"
   },
   "source": [
    "Increase window from 2 to 5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LS3LTqFeHeKU"
   },
   "source": [
    "Skip-gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Xt6ocoVG4lv",
    "outputId": "5642d190-a168-4561-8e86-58b4216aa93c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.54 s, sys: 7.52 ms, total: 1.55 s\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "workers = num_processors - 1\n",
    "\n",
    "sg_model = gensim.models.Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=1,                    # 1 = Skip-gram\n",
    "    compute_loss=True,\n",
    "    workers=workers,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sitRDfpkHFEL",
    "outputId": "cc9d2179-c4cc-48f6-e199-4f0dc2957bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2466048.75\n",
      "CPU times: user 106 µs, sys: 12 µs, total: 118 µs\n",
      "Wall time: 124 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# getting the training loss value\n",
    "sg_training_loss = sg_model.get_latest_training_loss()\n",
    "print(sg_training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwNkKW8QHhBs"
   },
   "source": [
    "CBOW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XRtoCJmj6Ltx",
    "outputId": "ca5f121d-94f1-4e23-a085-4427bd83ee52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 390 ms, sys: 6.53 ms, total: 397 ms\n",
      "Wall time: 396 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "workers = num_processors - 1\n",
    "\n",
    "cbow_model = gensim.models.Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=0,                    # 0 = CBOW\n",
    "    compute_loss=True,\n",
    "    workers=workers,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-r9Eyv0UHNId",
    "outputId": "012850c2-2532-423a-ca17-7ac6218c924e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536882.5625\n",
      "CPU times: user 86 µs, sys: 0 ns, total: 86 µs\n",
      "Wall time: 91.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# getting the training loss value\n",
    "cbow_training_loss = cbow_model.get_latest_training_loss()\n",
    "print(cbow_training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bx5fNLkMNuwx"
   },
   "source": [
    "Similar words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AZWvzdp4NwIT",
    "outputId": "3f1d6d25-0145-4151-89d8-0f0b421ef20c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"sg_cbow_similar_words_df(sg_model, cbow_model, target_words, topn)\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"Target Word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"government\",\n          \"tyrant\",\n          \"political\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SG Top1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"ends (0.967691)\",\n          \"past (0.995925)\",\n          \"member (0.983046)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SG Top2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"form (0.966334)\",\n          \"saul (0.995903)\",\n          \"whereby (0.982512)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SG Top3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"representatives (0.966092)\",\n          \"ruin (0.995603)\",\n          \"far (0.981715)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SG Top4\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"necessary (0.964567)\",\n          \"next (0.995078)\",\n          \"entered (0.981245)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SG Top5\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"require (0.964526)\",\n          \"nations (0.995060)\",\n          \"ends (0.980741)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CBOW Top1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"upon (0.999645)\",\n          \"way (0.998702)\",\n          \"due (0.999305)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CBOW Top2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"together (0.999551)\",\n          \"long (0.998604)\",\n          \"commonwealth (0.999290)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CBOW Top3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"set (0.999532)\",\n          \"present (0.998573)\",\n          \"king (0.999277)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CBOW Top4\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"either (0.999526)\",\n          \"support (0.998556)\",\n          \"together (0.999273)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CBOW Top5\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"new (0.999516)\",\n          \"hundred (0.998553)\",\n          \"better (0.999264)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-8c4c1460-8a53-44b1-8a45-7bed905a9629\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target Word</th>\n",
       "      <th>SG Top1</th>\n",
       "      <th>SG Top2</th>\n",
       "      <th>SG Top3</th>\n",
       "      <th>SG Top4</th>\n",
       "      <th>SG Top5</th>\n",
       "      <th>CBOW Top1</th>\n",
       "      <th>CBOW Top2</th>\n",
       "      <th>CBOW Top3</th>\n",
       "      <th>CBOW Top4</th>\n",
       "      <th>CBOW Top5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>government</td>\n",
       "      <td>ends (0.967691)</td>\n",
       "      <td>form (0.966334)</td>\n",
       "      <td>representatives (0.966092)</td>\n",
       "      <td>necessary (0.964567)</td>\n",
       "      <td>require (0.964526)</td>\n",
       "      <td>upon (0.999645)</td>\n",
       "      <td>together (0.999551)</td>\n",
       "      <td>set (0.999532)</td>\n",
       "      <td>either (0.999526)</td>\n",
       "      <td>new (0.999516)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>slavery</td>\n",
       "      <td>forfeited (0.996781)</td>\n",
       "      <td>subjected (0.996269)</td>\n",
       "      <td>sovereignty (0.995833)</td>\n",
       "      <td>fortunes (0.995743)</td>\n",
       "      <td>proved (0.995729)</td>\n",
       "      <td>able (0.999205)</td>\n",
       "      <td>violence (0.999192)</td>\n",
       "      <td>set (0.999156)</td>\n",
       "      <td>bound (0.999149)</td>\n",
       "      <td>execution (0.999146)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>property</td>\n",
       "      <td>gave (0.968743)</td>\n",
       "      <td>without (0.967010)</td>\n",
       "      <td>could (0.966045)</td>\n",
       "      <td>consent (0.963188)</td>\n",
       "      <td>appropriate (0.961546)</td>\n",
       "      <td>left (0.999470)</td>\n",
       "      <td>without (0.999435)</td>\n",
       "      <td>made (0.999414)</td>\n",
       "      <td>labour (0.999387)</td>\n",
       "      <td>thereby (0.999384)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>war</td>\n",
       "      <td>state (0.952422)</td>\n",
       "      <td>puts (0.950416)</td>\n",
       "      <td>right (0.926274)</td>\n",
       "      <td>man (0.923274)</td>\n",
       "      <td>force (0.916896)</td>\n",
       "      <td>people (0.999341)</td>\n",
       "      <td>mankind (0.999280)</td>\n",
       "      <td>rule (0.999269)</td>\n",
       "      <td>though (0.999256)</td>\n",
       "      <td>still (0.999252)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>state</td>\n",
       "      <td>puts (0.964353)</td>\n",
       "      <td>nature (0.959098)</td>\n",
       "      <td>war (0.952422)</td>\n",
       "      <td>man (0.943646)</td>\n",
       "      <td>whether (0.938932)</td>\n",
       "      <td>man (0.999139)</td>\n",
       "      <td>makes (0.999062)</td>\n",
       "      <td>yet (0.999040)</td>\n",
       "      <td>law (0.999034)</td>\n",
       "      <td>war (0.999026)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>love</td>\n",
       "      <td>principles (0.996739)</td>\n",
       "      <td>food (0.996523)</td>\n",
       "      <td>notwithstanding (0.996247)</td>\n",
       "      <td>accordingly (0.996131)</td>\n",
       "      <td>evil (0.996075)</td>\n",
       "      <td>long (0.999037)</td>\n",
       "      <td>great (0.999034)</td>\n",
       "      <td>far (0.999028)</td>\n",
       "      <td>certain (0.999028)</td>\n",
       "      <td>received (0.999024)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>land</td>\n",
       "      <td>value (0.957648)</td>\n",
       "      <td>acres (0.942017)</td>\n",
       "      <td>part (0.939996)</td>\n",
       "      <td>money (0.938780)</td>\n",
       "      <td>would (0.938316)</td>\n",
       "      <td>made (0.999466)</td>\n",
       "      <td>thing (0.999465)</td>\n",
       "      <td>together (0.999457)</td>\n",
       "      <td>might (0.999451)</td>\n",
       "      <td>yet (0.999436)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owner</td>\n",
       "      <td>anyone (0.996454)</td>\n",
       "      <td>tax (0.995962)</td>\n",
       "      <td>including (0.995249)</td>\n",
       "      <td>distribute (0.994986)</td>\n",
       "      <td>section (0.994975)</td>\n",
       "      <td>set (0.997984)</td>\n",
       "      <td>agree (0.997965)</td>\n",
       "      <td>possession (0.997945)</td>\n",
       "      <td>agreement (0.997850)</td>\n",
       "      <td>ends (0.997840)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>child</td>\n",
       "      <td>honour (0.992755)</td>\n",
       "      <td>subjection (0.992333)</td>\n",
       "      <td>mother (0.991918)</td>\n",
       "      <td>minority (0.990299)</td>\n",
       "      <td>education (0.987576)</td>\n",
       "      <td>either (0.999485)</td>\n",
       "      <td>find (0.999465)</td>\n",
       "      <td>see (0.999430)</td>\n",
       "      <td>respect (0.999420)</td>\n",
       "      <td>things (0.999419)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>history</td>\n",
       "      <td>examples (0.994785)</td>\n",
       "      <td>families (0.994317)</td>\n",
       "      <td>instances (0.994194)</td>\n",
       "      <td>practice (0.994048)</td>\n",
       "      <td>speak (0.994033)</td>\n",
       "      <td>must (0.999399)</td>\n",
       "      <td>beginning (0.999379)</td>\n",
       "      <td>way (0.999350)</td>\n",
       "      <td>thing (0.999348)</td>\n",
       "      <td>condition (0.999327)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>health</td>\n",
       "      <td>bare (0.996387)</td>\n",
       "      <td>perish (0.996383)</td>\n",
       "      <td>france (0.996362)</td>\n",
       "      <td>turn (0.996252)</td>\n",
       "      <td>freemen (0.996066)</td>\n",
       "      <td>natural (0.996133)</td>\n",
       "      <td>sword (0.996126)</td>\n",
       "      <td>god (0.996077)</td>\n",
       "      <td>age (0.996070)</td>\n",
       "      <td>according (0.996064)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>legislative</td>\n",
       "      <td>supreme (0.988922)</td>\n",
       "      <td>executive (0.978491)</td>\n",
       "      <td>trust (0.955374)</td>\n",
       "      <td>commonwealth (0.954141)</td>\n",
       "      <td>hands (0.952112)</td>\n",
       "      <td>commonwealth (0.999423)</td>\n",
       "      <td>still (0.999379)</td>\n",
       "      <td>supreme (0.999366)</td>\n",
       "      <td>shall (0.999325)</td>\n",
       "      <td>upon (0.999324)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>federative</td>\n",
       "      <td>convoking (0.996836)</td>\n",
       "      <td>changed (0.995900)</td>\n",
       "      <td>alter (0.995085)</td>\n",
       "      <td>transfer (0.994528)</td>\n",
       "      <td>acts (0.994436)</td>\n",
       "      <td>true (0.997184)</td>\n",
       "      <td>many (0.997134)</td>\n",
       "      <td>since (0.997108)</td>\n",
       "      <td>rules (0.997108)</td>\n",
       "      <td>force (0.997032)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>prerogative</td>\n",
       "      <td>people (0.983521)</td>\n",
       "      <td>always (0.980828)</td>\n",
       "      <td>contrary (0.974787)</td>\n",
       "      <td>ends (0.974700)</td>\n",
       "      <td>safety (0.973549)</td>\n",
       "      <td>people (0.999486)</td>\n",
       "      <td>governments (0.999418)</td>\n",
       "      <td>either (0.999398)</td>\n",
       "      <td>think (0.999398)</td>\n",
       "      <td>find (0.999387)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>paternal</td>\n",
       "      <td>command (0.983279)</td>\n",
       "      <td>family (0.981413)</td>\n",
       "      <td>death (0.980103)</td>\n",
       "      <td>despotical (0.979328)</td>\n",
       "      <td>obedience (0.979136)</td>\n",
       "      <td>either (0.999489)</td>\n",
       "      <td>nothing (0.999422)</td>\n",
       "      <td>several (0.999422)</td>\n",
       "      <td>care (0.999400)</td>\n",
       "      <td>provide (0.999391)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>political</td>\n",
       "      <td>member (0.983046)</td>\n",
       "      <td>whereby (0.982512)</td>\n",
       "      <td>far (0.981715)</td>\n",
       "      <td>entered (0.981245)</td>\n",
       "      <td>ends (0.980741)</td>\n",
       "      <td>due (0.999305)</td>\n",
       "      <td>commonwealth (0.999290)</td>\n",
       "      <td>king (0.999277)</td>\n",
       "      <td>together (0.999273)</td>\n",
       "      <td>better (0.999264)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>power</td>\n",
       "      <td>hands (0.950799)</td>\n",
       "      <td>put (0.939602)</td>\n",
       "      <td>executive (0.937071)</td>\n",
       "      <td>supreme (0.928534)</td>\n",
       "      <td>arbitrary (0.920730)</td>\n",
       "      <td>commonwealth (0.999247)</td>\n",
       "      <td>prince (0.999215)</td>\n",
       "      <td>put (0.999212)</td>\n",
       "      <td>hands (0.999201)</td>\n",
       "      <td>legislative (0.999188)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tyrant</td>\n",
       "      <td>past (0.995925)</td>\n",
       "      <td>saul (0.995903)</td>\n",
       "      <td>ruin (0.995603)</td>\n",
       "      <td>next (0.995078)</td>\n",
       "      <td>nations (0.995060)</td>\n",
       "      <td>way (0.998702)</td>\n",
       "      <td>long (0.998604)</td>\n",
       "      <td>present (0.998573)</td>\n",
       "      <td>support (0.998556)</td>\n",
       "      <td>hundred (0.998553)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>king</td>\n",
       "      <td>want (0.972242)</td>\n",
       "      <td>commission (0.970802)</td>\n",
       "      <td>though (0.968051)</td>\n",
       "      <td>lawful (0.967909)</td>\n",
       "      <td>lord (0.967385)</td>\n",
       "      <td>together (0.999611)</td>\n",
       "      <td>case (0.999576)</td>\n",
       "      <td>done (0.999573)</td>\n",
       "      <td>age (0.999561)</td>\n",
       "      <td>must (0.999554)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dissolution</td>\n",
       "      <td>forms (0.996760)</td>\n",
       "      <td>separate (0.996613)</td>\n",
       "      <td>chuse (0.996358)</td>\n",
       "      <td>successors (0.996335)</td>\n",
       "      <td>governed (0.996183)</td>\n",
       "      <td>presently (0.998400)</td>\n",
       "      <td>prince (0.998169)</td>\n",
       "      <td>think (0.998143)</td>\n",
       "      <td>form (0.998116)</td>\n",
       "      <td>either (0.998096)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c4c1460-8a53-44b1-8a45-7bed905a9629')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-8c4c1460-8a53-44b1-8a45-7bed905a9629 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-8c4c1460-8a53-44b1-8a45-7bed905a9629');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-91742376-74ca-47ae-b244-bacda949e1c9\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-91742376-74ca-47ae-b244-bacda949e1c9')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-91742376-74ca-47ae-b244-bacda949e1c9 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "    Target Word                SG Top1                SG Top2  \\\n",
       "0    government        ends (0.967691)        form (0.966334)   \n",
       "1       slavery   forfeited (0.996781)   subjected (0.996269)   \n",
       "2      property        gave (0.968743)     without (0.967010)   \n",
       "3           war       state (0.952422)        puts (0.950416)   \n",
       "4         state        puts (0.964353)      nature (0.959098)   \n",
       "5          love  principles (0.996739)        food (0.996523)   \n",
       "6          land       value (0.957648)       acres (0.942017)   \n",
       "7         owner      anyone (0.996454)         tax (0.995962)   \n",
       "8         child      honour (0.992755)  subjection (0.992333)   \n",
       "9       history    examples (0.994785)    families (0.994317)   \n",
       "10       health        bare (0.996387)      perish (0.996383)   \n",
       "11  legislative     supreme (0.988922)   executive (0.978491)   \n",
       "12   federative   convoking (0.996836)     changed (0.995900)   \n",
       "13  prerogative      people (0.983521)      always (0.980828)   \n",
       "14     paternal     command (0.983279)      family (0.981413)   \n",
       "15    political      member (0.983046)     whereby (0.982512)   \n",
       "16        power       hands (0.950799)         put (0.939602)   \n",
       "17       tyrant        past (0.995925)        saul (0.995903)   \n",
       "18         king        want (0.972242)  commission (0.970802)   \n",
       "19  dissolution       forms (0.996760)    separate (0.996613)   \n",
       "\n",
       "                       SG Top3                  SG Top4  \\\n",
       "0   representatives (0.966092)     necessary (0.964567)   \n",
       "1       sovereignty (0.995833)      fortunes (0.995743)   \n",
       "2             could (0.966045)       consent (0.963188)   \n",
       "3             right (0.926274)           man (0.923274)   \n",
       "4               war (0.952422)           man (0.943646)   \n",
       "5   notwithstanding (0.996247)   accordingly (0.996131)   \n",
       "6              part (0.939996)         money (0.938780)   \n",
       "7         including (0.995249)    distribute (0.994986)   \n",
       "8            mother (0.991918)      minority (0.990299)   \n",
       "9         instances (0.994194)      practice (0.994048)   \n",
       "10           france (0.996362)          turn (0.996252)   \n",
       "11            trust (0.955374)  commonwealth (0.954141)   \n",
       "12            alter (0.995085)      transfer (0.994528)   \n",
       "13         contrary (0.974787)          ends (0.974700)   \n",
       "14            death (0.980103)    despotical (0.979328)   \n",
       "15              far (0.981715)       entered (0.981245)   \n",
       "16        executive (0.937071)       supreme (0.928534)   \n",
       "17             ruin (0.995603)          next (0.995078)   \n",
       "18           though (0.968051)        lawful (0.967909)   \n",
       "19            chuse (0.996358)    successors (0.996335)   \n",
       "\n",
       "                   SG Top5                CBOW Top1                CBOW Top2  \\\n",
       "0       require (0.964526)          upon (0.999645)      together (0.999551)   \n",
       "1        proved (0.995729)          able (0.999205)      violence (0.999192)   \n",
       "2   appropriate (0.961546)          left (0.999470)       without (0.999435)   \n",
       "3         force (0.916896)        people (0.999341)       mankind (0.999280)   \n",
       "4       whether (0.938932)           man (0.999139)         makes (0.999062)   \n",
       "5          evil (0.996075)          long (0.999037)         great (0.999034)   \n",
       "6         would (0.938316)          made (0.999466)         thing (0.999465)   \n",
       "7       section (0.994975)           set (0.997984)         agree (0.997965)   \n",
       "8     education (0.987576)        either (0.999485)          find (0.999465)   \n",
       "9         speak (0.994033)          must (0.999399)     beginning (0.999379)   \n",
       "10      freemen (0.996066)       natural (0.996133)         sword (0.996126)   \n",
       "11        hands (0.952112)  commonwealth (0.999423)         still (0.999379)   \n",
       "12         acts (0.994436)          true (0.997184)          many (0.997134)   \n",
       "13       safety (0.973549)        people (0.999486)   governments (0.999418)   \n",
       "14    obedience (0.979136)        either (0.999489)       nothing (0.999422)   \n",
       "15         ends (0.980741)           due (0.999305)  commonwealth (0.999290)   \n",
       "16    arbitrary (0.920730)  commonwealth (0.999247)        prince (0.999215)   \n",
       "17      nations (0.995060)           way (0.998702)          long (0.998604)   \n",
       "18         lord (0.967385)      together (0.999611)          case (0.999576)   \n",
       "19     governed (0.996183)     presently (0.998400)        prince (0.998169)   \n",
       "\n",
       "                CBOW Top3             CBOW Top4               CBOW Top5  \n",
       "0          set (0.999532)     either (0.999526)          new (0.999516)  \n",
       "1          set (0.999156)      bound (0.999149)    execution (0.999146)  \n",
       "2         made (0.999414)     labour (0.999387)      thereby (0.999384)  \n",
       "3         rule (0.999269)     though (0.999256)        still (0.999252)  \n",
       "4          yet (0.999040)        law (0.999034)          war (0.999026)  \n",
       "5          far (0.999028)    certain (0.999028)     received (0.999024)  \n",
       "6     together (0.999457)      might (0.999451)          yet (0.999436)  \n",
       "7   possession (0.997945)  agreement (0.997850)         ends (0.997840)  \n",
       "8          see (0.999430)    respect (0.999420)       things (0.999419)  \n",
       "9          way (0.999350)      thing (0.999348)    condition (0.999327)  \n",
       "10         god (0.996077)        age (0.996070)    according (0.996064)  \n",
       "11     supreme (0.999366)      shall (0.999325)         upon (0.999324)  \n",
       "12       since (0.997108)      rules (0.997108)        force (0.997032)  \n",
       "13      either (0.999398)      think (0.999398)         find (0.999387)  \n",
       "14     several (0.999422)       care (0.999400)      provide (0.999391)  \n",
       "15        king (0.999277)   together (0.999273)       better (0.999264)  \n",
       "16         put (0.999212)      hands (0.999201)  legislative (0.999188)  \n",
       "17     present (0.998573)    support (0.998556)      hundred (0.998553)  \n",
       "18        done (0.999573)        age (0.999561)         must (0.999554)  \n",
       "19       think (0.998143)       form (0.998116)       either (0.998096)  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_cbow_similar_words_df(sg_model, cbow_model, target_words, topn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YJdks2zIJCz"
   },
   "source": [
    "# Further Exploration\n",
    "\n",
    "When increasing the window size from 2 to 5, the **Skip-gram model's training loss increased** to 2,466,048.75, while **CBOW's remained lower at 536,882.5625**. This difference is expected due to Skip-gram making more individual predictions as the window expands. However, despite the higher loss, Skip-gram shows clear semantic improvements with the larger context window, while CBOW's results remain largely unchanged. This highlights Skip-gram's advantage in capturing richer relationships, even with increased computational cost.\n",
    "\n",
    "To be more specific, after increasing the context window, the **Skip-gram model shows noticeably improved semantic results**. For example, the word \"slavery\" now returns more relevant terms such as \"forfeited\", \"subjected\", and \"sovereignty\", which align well with historical and political themes. Similarly, words like \"government\", \"war\", and \"owner\" are now linked to stronger and more interpretable semantic neighbors.\n",
    "\n",
    "In contrast, **CBOW's outputs remain relatively unchanged**. It continues to return frequent or structurally common words such as \"able\", \"together\", \"thing\", and \"must\", with limited variation across different target terms. This reflects the limitations of CBOW's context-averaging approach, which tends to smooth over the semantic specificity gained from a wider context.\n",
    "\n",
    "These findings reinforce that **Skip-gram benefits more from an increased window size**. Although it incurs **higher computational cost and loss**, the trade-off results in richer and more interpretable word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Fz7mGwnpHQ3i"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
